{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9317887,"sourceType":"datasetVersion","datasetId":5643951},{"sourceId":108099,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":90542,"modelId":114758}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook for LLM workshop","metadata":{}},{"cell_type":"markdown","source":"## Part 1. Examining and fine-tuning BERT\n\nBased on a tutorial from [Kaggle](https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation)\n","metadata":{}},{"cell_type":"code","source":"# Import Python libraries\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:13.752374Z","iopub.execute_input":"2024-09-04T15:28:13.753354Z","iopub.status.idle":"2024-09-04T15:28:16.506477Z","shell.execute_reply.started":"2024-09-04T15:28:13.753306Z","shell.execute_reply":"2024-09-04T15:28:16.505390Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Getting BERT and its tokenizer","metadata":{}},{"cell_type":"code","source":"# Import the Transformers library and download BERT\n\nfrom transformers import AutoModel, BertTokenizerFast, AdamW\n\nbert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:16.508104Z","iopub.execute_input":"2024-09-04T15:28:16.508586Z","iopub.status.idle":"2024-09-04T15:28:23.589768Z","shell.execute_reply.started":"2024-09-04T15:28:16.508550Z","shell.execute_reply":"2024-09-04T15:28:23.588754Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa40e931f8a4883b854a72ee7ad5c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a20b1fa89a48849dd012bc991c0a01"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d1aefb38c14eef9326455dedc7bb90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b710e367e0864b36a1eea37d78f3961e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33a6ec08ce2437098691d5e23e6bd00"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"bert","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:23.590832Z","iopub.execute_input":"2024-09-04T15:28:23.591280Z","iopub.status.idle":"2024-09-04T15:28:23.600132Z","shell.execute_reply.started":"2024-09-04T15:28:23.591246Z","shell.execute_reply":"2024-09-04T15:28:23.599203Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:23.602403Z","iopub.execute_input":"2024-09-04T15:28:23.602779Z","iopub.status.idle":"2024-09-04T15:28:23.615606Z","shell.execute_reply.started":"2024-09-04T15:28:23.602745Z","shell.execute_reply":"2024-09-04T15:28:23.614725Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Defining the BERT-based sentence classifier architecture","metadata":{}},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n        super(BERT_Arch, self).__init__()\n\n        # BERT part\n        self.bert = bert \n        \n        # dropout layer (random removing of components during training for improving performance)\n        self.dropout = nn.Dropout(0.1)\n      \n        # relu activation function\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n      \n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,2)\n\n        #softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    # define the forward pass\n    def forward(self, sent_id, mask):\n        \n        # pass the inputs to the model  \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n\n        # run BERT output through dense layer 1\n        x = self.fc1(cls_hs)\n\n        # apply relu activation function\n        x = self.relu(x)\n\n        # apply dropout\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n      \n        # apply softmax activation (get probability distribution across target classes)\n        x = self.softmax(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:23.616842Z","iopub.execute_input":"2024-09-04T15:28:23.617224Z","iopub.status.idle":"2024-09-04T15:28:23.626653Z","shell.execute_reply.started":"2024-09-04T15:28:23.617181Z","shell.execute_reply":"2024-09-04T15:28:23.625782Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = BERT_Arch(bert)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:23.627757Z","iopub.execute_input":"2024-09-04T15:28:23.628043Z","iopub.status.idle":"2024-09-04T15:28:23.647101Z","shell.execute_reply.started":"2024-09-04T15:28:23.628012Z","shell.execute_reply":"2024-09-04T15:28:23.646076Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Put model to GPU if available\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:23.648362Z","iopub.execute_input":"2024-09-04T15:28:23.648959Z","iopub.status.idle":"2024-09-04T15:28:24.065648Z","shell.execute_reply.started":"2024-09-04T15:28:23.648916Z","shell.execute_reply":"2024-09-04T15:28:24.064830Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.066796Z","iopub.execute_input":"2024-09-04T15:28:24.067126Z","iopub.status.idle":"2024-09-04T15:28:24.074921Z","shell.execute_reply.started":"2024-09-04T15:28:24.067091Z","shell.execute_reply":"2024-09-04T15:28:24.074030Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BERT_Arch(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (relu): ReLU()\n  (fc1): Linear(in_features=768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (softmax): LogSoftmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Getting the dataset","metadata":{}},{"cell_type":"code","source":"# use pre-labeled spam detection dataset\ndf = pd.read_csv(\"/kaggle/input/spamdata-v2/spamdata_v2.csv\")\n\n# split train dataset into train, validation and test sets\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n                                                                    random_state=2024, \n                                                                    test_size=0.3, \n                                                                    stratify=df['label'])\n\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2024, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)\n\n# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = 25,\n    padding='max_length',\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = 25,\n    padding='max_length',\n    truncation=True\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = 25,\n    padding='max_length',\n    truncation=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.076272Z","iopub.execute_input":"2024-09-04T15:28:24.076647Z","iopub.status.idle":"2024-09-04T15:28:24.743038Z","shell.execute_reply.started":"2024-09-04T15:28:24.076602Z","shell.execute_reply":"2024-09-04T15:28:24.742069Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# convert lists to tensors\n\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.746556Z","iopub.execute_input":"2024-09-04T15:28:24.746877Z","iopub.status.idle":"2024-09-04T15:28:24.871050Z","shell.execute_reply.started":"2024-09-04T15:28:24.746844Z","shell.execute_reply":"2024-09-04T15:28:24.870078Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# train_mask is used for ignoring input padding in attention\n\nprint(tokenizer.decode(train_seq[0]))\nprint(train_mask[0])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-04T15:28:24.872479Z","iopub.execute_input":"2024-09-04T15:28:24.873068Z","iopub.status.idle":"2024-09-04T15:28:24.885214Z","shell.execute_reply.started":"2024-09-04T15:28:24.873017Z","shell.execute_reply":"2024-09-04T15:28:24.884079Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[CLS] u can call now... [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training the classifier","metadata":{}},{"cell_type":"code","source":"# define a batch size\nbatch_size = 32\n\n# wrap tensors for training data\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for randomly sampling data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for training data\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors for validation data\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for randomly sampling data during validation\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.886482Z","iopub.execute_input":"2024-09-04T15:28:24.887203Z","iopub.status.idle":"2024-09-04T15:28:24.894216Z","shell.execute_reply.started":"2024-09-04T15:28:24.887136Z","shell.execute_reply":"2024-09-04T15:28:24.893217Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# freeze all BERT parameters (training only linear layers outside of BERT)\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.895515Z","iopub.execute_input":"2024-09-04T15:28:24.896136Z","iopub.status.idle":"2024-09-04T15:28:24.912429Z","shell.execute_reply.started":"2024-09-04T15:28:24.896100Z","shell.execute_reply":"2024-09-04T15:28:24.911638Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# define the optimizer\noptimizer = AdamW(model.parameters(),lr = 1e-5) \n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n\n# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.913601Z","iopub.execute_input":"2024-09-04T15:28:24.913878Z","iopub.status.idle":"2024-09-04T15:28:25.754379Z","shell.execute_reply.started":"2024-09-04T15:28:24.913847Z","shell.execute_reply":"2024-09-04T15:28:25.753620Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# function to train the model\ndef train():\n\n    # put model in training mode (apply Dropout)\n    model.train()\n\n    # initialize loss and accuracy\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save model predictions\n    total_preds = []\n  \n    # iterate over batches\n    for step,batch in enumerate(train_dataloader):\n        \n        # print progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n        \n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n \n        sent_id, mask, labels = batch\n        \n        # clear previously calculated gradients \n        model.zero_grad()        \n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(train_dataloader)\n  \n      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n      # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds\n\n\n# function for evaluating the model\ndef evaluate():\n    \n    print(\"\\nEvaluating...\")\n  \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step,batch in enumerate(val_dataloader):\n        \n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            \n            # # Calculate elapsed time in minutes.\n            # elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n            \n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:25.755625Z","iopub.execute_input":"2024-09-04T15:28:25.756118Z","iopub.status.idle":"2024-09-04T15:28:25.769901Z","shell.execute_reply.started":"2024-09-04T15:28:25.756083Z","shell.execute_reply":"2024-09-04T15:28:25.769132Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n# for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    # train model\n    train_loss, _ = train()\n    \n    # evaluate model\n    valid_loss, _ = evaluate()\n    \n    # save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), '/kaggle/working/saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')\n\n\npath = '/kaggle/working/saved_weights.pt'\nmodel.load_state_dict(torch.load(path))\n\nwith torch.no_grad():\n    preds = model(test_seq.to(device), test_mask.to(device))\n    preds = preds.detach().cpu().numpy()\n\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))\n\n# free cached GPU memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:25.771062Z","iopub.execute_input":"2024-09-04T15:28:25.771739Z","iopub.status.idle":"2024-09-04T15:29:45.432709Z","shell.execute_reply.started":"2024-09-04T15:28:25.771695Z","shell.execute_reply":"2024-09-04T15:29:45.431692Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.691\nValidation Loss: 0.664\n\n Epoch 2 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.648\nValidation Loss: 0.628\n\n Epoch 3 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.618\nValidation Loss: 0.600\n\n Epoch 4 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.592\nValidation Loss: 0.576\n\n Epoch 5 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.570\nValidation Loss: 0.558\n\n Epoch 6 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.547\nValidation Loss: 0.529\n\n Epoch 7 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.530\nValidation Loss: 0.509\n\n Epoch 8 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.513\nValidation Loss: 0.495\n\n Epoch 9 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.489\nValidation Loss: 0.479\n\n Epoch 10 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.476\nValidation Loss: 0.462\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_146/3071509593.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(path))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.98      0.82      0.89       724\n           1       0.44      0.90      0.59       112\n\n    accuracy                           0.83       836\n   macro avg       0.71      0.86      0.74       836\nweighted avg       0.91      0.83      0.85       836\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## loading saved model\n\nimport torch\n\nsaved_model_path = '/kaggle/input/saved_weights.pt/transformers/default/1/saved_weights.pt'\nmodel = torch.load(saved_model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T16:56:56.531114Z","iopub.execute_input":"2024-09-05T16:56:56.531830Z","iopub.status.idle":"2024-09-05T16:57:07.414490Z","shell.execute_reply.started":"2024-09-05T16:56:56.531775Z","shell.execute_reply":"2024-09-05T16:57:07.413442Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3878222882.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(saved_model_path)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Part 2. Sentence similarities with SentenceBERT\n\nBased on [SentenceBERT documentation](https://sbert.net/)","metadata":{}},{"cell_type":"code","source":"# !pip install sentence-transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:57:00.365140Z","iopub.execute_input":"2024-09-05T15:57:00.365541Z","iopub.status.idle":"2024-09-05T15:57:00.370763Z","shell.execute_reply.started":"2024-09-05T15:57:00.365503Z","shell.execute_reply":"2024-09-05T15:57:00.369757Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:57:04.248877Z","iopub.execute_input":"2024-09-05T15:57:04.249259Z","iopub.status.idle":"2024-09-05T15:57:23.744030Z","shell.execute_reply.started":"2024-09-05T15:57:04.249222Z","shell.execute_reply":"2024-09-05T15:57:23.743239Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"}]},{"cell_type":"code","source":"model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:57:23.745803Z","iopub.execute_input":"2024-09-05T15:57:23.746941Z","iopub.status.idle":"2024-09-05T15:57:26.357487Z","shell.execute_reply.started":"2024-09-05T15:57:23.746892Z","shell.execute_reply":"2024-09-05T15:57:26.356677Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5790d72033d24b9a84fa4d823997d5e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea1337fc94a4cb39ec468501e42f4e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0f15a354d6e42a289ee1014ae59227f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50276e0b65440adb5ca1fac12684be7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528cd174c6644dcc8d3a327f78d982d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1fbb19ee4849ac883c5d1c89014f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d16d7ef0a5e4887b0321ecc7167d504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9474ea3fa734e5d957fff39ea4db391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb197d46d1e549a393c3ce51931a5d13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a2f3db8f2e49d582f0dc22f73e0470"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347d42c32a704a908eed3a18da31ac51"}},"metadata":{}}]},{"cell_type":"code","source":"# sentences to encode\nsentences = [\n    \"I am happy.\",\n    \"I am sad.\",\n    \"I am content.\",\n    \"I am not happy.\",\n    \"I am not sad.\"\n]\n\n# embeddings\nembeddings = model.encode(sentences)\n\n# embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:57:40.057022Z","iopub.execute_input":"2024-09-05T15:57:40.057420Z","iopub.status.idle":"2024-09-05T15:57:40.843922Z","shell.execute_reply.started":"2024-09-05T15:57:40.057369Z","shell.execute_reply":"2024-09-05T15:57:40.842862Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a375866e532e4263a1b9ac5b35fc6add"}},"metadata":{}},{"name":"stdout","text":"tensor([[1.0000, 0.4128, 0.5024, 0.7230, 0.4712],\n        [0.4128, 1.0000, 0.3096, 0.4911, 0.7424],\n        [0.5024, 0.3096, 1.0000, 0.4433, 0.3470],\n        [0.7230, 0.4911, 0.4433, 1.0000, 0.6147],\n        [0.4712, 0.7424, 0.3470, 0.6147, 1.0000]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Part 3. Examining Llama2 predictions","metadata":{}},{"cell_type":"code","source":"# !pip install bitsandbytes\n!pip install -U bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T16:57:53.441305Z","iopub.execute_input":"2024-09-05T16:57:53.441811Z","iopub.status.idle":"2024-09-05T16:58:18.751980Z","shell.execute_reply.started":"2024-09-05T16:57:53.441773Z","shell.execute_reply":"2024-09-05T16:58:18.750837Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import bitsandbytes as bnb\nprint(bnb.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T16:23:36.514091Z","iopub.execute_input":"2024-09-05T16:23:36.514470Z","iopub.status.idle":"2024-09-05T16:23:36.623543Z","shell.execute_reply.started":"2024-09-05T16:23:36.514436Z","shell.execute_reply":"2024-09-05T16:23:36.622551Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0.43.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-09-05T16:58:25.868929Z","iopub.execute_input":"2024-09-05T16:58:25.869609Z","iopub.status.idle":"2024-09-05T16:58:27.779705Z","shell.execute_reply.started":"2024-09-05T16:58:25.869566Z","shell.execute_reply":"2024-09-05T16:58:27.778884Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nquant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False)\n\n# Pre-trained LLAMA2 from Hugging Face hub\nbase_model = \"NousResearch/Llama-2-7b-chat-hf\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=quant_config, device_map={\"\": 0})\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-09-05T16:58:36.419284Z","iopub.execute_input":"2024-09-05T16:58:36.420238Z","iopub.status.idle":"2024-09-05T17:00:00.872222Z","shell.execute_reply.started":"2024-09-05T16:58:36.420194Z","shell.execute_reply":"2024-09-05T17:00:00.871200Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d165b6ea454c46e49b7004b5cbc0aef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b66df0f268842af9a3165e588c6e9e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7324c748bea94ac19dd54e559cc3b41a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee633b35aaf45f8beffab4e7e73bce9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c2e065abc74ee7befcc53b71399f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e80487d093542c68308a5579ecae404"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a851d902e7724ce881deceb72e0cfb82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48adce5fd6d34237945ec6bdd83389f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac324407ca64aa3ba7d196b3f1085a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89e70b294b7544ceb35d42e9e1f28223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb9d28b65184a06841a9b69a3d00515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aef1f387c0741a6adcd8b74add2e6b6"}},"metadata":{}}]},{"cell_type":"code","source":"# examining model structure\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:02:56.738477Z","iopub.execute_input":"2024-09-05T17:02:56.739022Z","iopub.status.idle":"2024-09-05T17:02:56.749438Z","shell.execute_reply.started":"2024-09-05T17:02:56.738982Z","shell.execute_reply":"2024-09-05T17:02:56.748515Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# generating text with a prompt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprompt = \"Who is Ferdinand de Saussure?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\ngenerate_ids = model.generate(inputs.input_ids.to(device), max_length=100)\noutput = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:04:01.089307Z","iopub.execute_input":"2024-09-05T17:04:01.089719Z","iopub.status.idle":"2024-09-05T17:04:06.496553Z","shell.execute_reply.started":"2024-09-05T17:04:01.089681Z","shell.execute_reply":"2024-09-05T17:04:06.495500Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Who is Ferdinand de Saussure?\n hopefully, this will help you understand the significance of Ferdinand de Saussure in the history of linguistics.\nFerdinand de Saussure (1857-1913) was a Swiss linguist who is considered one of the founders of modern linguistics. He is best known for his work on the structure of language, which challenged traditional views of language and laid the groundwork for many of the theories and\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Getting probabilities for the next word\n\nBased on [this Reddit thread](https://www.reddit.com/r/LocalLLaMA/comments/1b6xbg9/displayingreturning_probabilitieslogprobs_of_next/?rdt=37394)","metadata":{}},{"cell_type":"code","source":"input_string = \"I am\"\n\n# tokenize input\ninput_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n\n# get model predictions (logits: values before they are turned into probabilities via softmax)\nlogits = model(input_ids).logits\n\n# last projection: predicted next word after input\nlogits = logits[-1, -1]\n\n# change logits to probabilities via softmax\nprobs = torch.nn.functional.softmax(logits, dim=-1)\n\n# top 10 most probable predicted words\nprobs, ids = torch.topk(probs, 10)\n\n# convert token ids to text tokens\ntexts = tokenizer.convert_ids_to_tokens(ids)\n\n# print probabilities + tokens\nfor prob, text in zip(probs, texts):\n    print(f\"{prob:.4f}: \\\"{text}\\\"\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:04:21.774603Z","iopub.execute_input":"2024-09-05T17:04:21.774995Z","iopub.status.idle":"2024-09-05T17:04:22.068003Z","shell.execute_reply.started":"2024-09-05T17:04:21.774957Z","shell.execute_reply":"2024-09-05T17:04:22.067009Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0.2253: \"▁a\"\n0.0631: \"▁pleased\"\n0.0531: \"▁not\"\n0.0503: \"▁so\"\n0.0454: \"▁thr\"\n0.0420: \"▁an\"\n0.0315: \"▁writing\"\n0.0255: \"▁excited\"\n0.0203: \"▁grateful\"\n0.0183: \"▁happy\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Getting predictions from different Llama2 layers\n\nBased on this [code](https://github.com/nrimsky/LM-exp/blob/main/intermediate_decoding/intermediate_decoding.ipynb) with an associated [post](https://www.lesswrong.com/posts/fJE6tscjGRPnK8C2C/decoding-intermediate-activations-in-llama-2-7b)","metadata":{}},{"cell_type":"code","source":"class AttnWrapper(torch.nn.Module):\n    def __init__(self, attn):\n        super().__init__()\n        self.attn = attn\n        self.activations = None\n        self.add_tensor = None\n\n    def forward(self, *args, **kwargs):\n        output = self.attn(*args, **kwargs)\n        if self.add_tensor is not None:\n            output = (output[0] + self.add_tensor,)+output[1:]\n        self.activations = output[0]\n        return output\n\n    def reset(self):\n        self.activations = None\n        self.add_tensor = None\n\n\nclass BlockOutputWrapper(torch.nn.Module):\n    def __init__(self, block, unembed_matrix, norm):\n        super().__init__()\n        self.block = block\n        self.unembed_matrix = unembed_matrix\n        self.norm = norm\n\n        self.block.self_attn = AttnWrapper(self.block.self_attn)\n        self.post_attention_layernorm = self.block.post_attention_layernorm\n\n        self.attn_mech_output_unembedded = None\n        self.intermediate_res_unembedded = None\n        self.mlp_output_unembedded = None\n        self.block_output_unembedded = None\n\n    def forward(self, *args, **kwargs):\n        output = self.block(*args, **kwargs)\n        self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n        attn_output = self.block.self_attn.activations\n        self.attn_mech_output_unembedded = self.unembed_matrix(self.norm(attn_output))\n        attn_output += args[0]\n        self.intermediate_res_unembedded = self.unembed_matrix(self.norm(attn_output))\n        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n        self.mlp_output_unembedded = self.unembed_matrix(self.norm(mlp_output))\n        return output\n\n    def attn_add_tensor(self, tensor):\n        self.block.self_attn.add_tensor = tensor\n\n    def reset(self):\n        self.block.self_attn.reset()\n\n    def get_attn_activations(self):\n        return self.block.self_attn.activations\n\n\nclass Llama7BHelper:\n    def __init__(self):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.tokenizer = tokenizer\n        self.model = model\n        for i, layer in enumerate(self.model.model.layers):\n            self.model.model.layers[i] = BlockOutputWrapper(layer, self.model.lm_head, self.model.model.norm)\n\n    def generate_text(self, prompt, max_length=100):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_length=max_length)\n        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\n    def get_logits(self, prompt):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        with torch.no_grad():\n          logits = self.model(inputs.input_ids.to(self.device)).logits\n          return logits\n\n    def set_add_attn_output(self, layer, add_output):\n        self.model.model.layers[layer].attn_add_tensor(add_output)\n\n    def get_attn_activations(self, layer):\n        return self.model.model.layers[layer].get_attn_activations()\n\n    def reset_all(self):\n        for layer in self.model.model.layers:\n            layer.reset()\n            \n    def print_decoded_activations(self, decoded_activations, label, topk=10):\n        softmaxed = torch.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n        values, indices = torch.topk(softmaxed, topk)\n        probs_percent = [int(v * 100) for v in values.tolist()]\n        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n        print(label, list(zip(tokens, probs_percent)))\n\n    def decode_all_layers(self, text, topk=10, print_attn_mech=True, print_intermediate_res=True, print_mlp=True, print_block=True):\n        self.get_logits(text)\n        for i, layer in enumerate(self.model.model.layers):\n            print(f'Layer {i}: Decoded intermediate outputs')\n            if print_attn_mech:\n                self.print_decoded_activations(layer.attn_mech_output_unembedded, 'Attention mechanism', topk=topk)\n            if print_intermediate_res:\n                self.print_decoded_activations(layer.intermediate_res_unembedded, 'Intermediate residual stream', topk=topk)\n            if print_mlp:\n                self.print_decoded_activations(layer.mlp_output_unembedded, 'MLP output', topk=topk)\n            if print_block:\n                self.print_decoded_activations(layer.block_output_unembedded, 'Block output', topk=topk)\n            print()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:04:57.929031Z","iopub.execute_input":"2024-09-05T17:04:57.929413Z","iopub.status.idle":"2024-09-05T17:04:57.953471Z","shell.execute_reply.started":"2024-09-05T17:04:57.929362Z","shell.execute_reply":"2024-09-05T17:04:57.952540Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"helper = Llama7BHelper()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:05:18.151162Z","iopub.execute_input":"2024-09-05T17:05:18.151652Z","iopub.status.idle":"2024-09-05T17:05:18.159478Z","shell.execute_reply.started":"2024-09-05T17:05:18.151611Z","shell.execute_reply":"2024-09-05T17:05:18.158447Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"helper.decode_all_layers('Colorless green')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:05:21.596997Z","iopub.execute_input":"2024-09-05T17:05:21.597428Z","iopub.status.idle":"2024-09-05T17:05:22.204046Z","shell.execute_reply.started":"2024-09-05T17:05:21.597373Z","shell.execute_reply":"2024-09-05T17:05:22.203123Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Layer 0: Decoded intermediate outputs\nAttention mechanism [('solution', 0), ('loose', 0), ('berga', 0), ('', 0), ('first', 0), ('Private', 0), ('itself', 0), ('applied', 0), ('h', 0), ('пута', 0)]\nIntermediate residual stream [('ery', 8), ('berga', 2), ('ো', 0), ('Fritz', 0), ('SM', 0), ('SM', 0), ('solution', 0), ('aram', 0), ('hill', 0), ('ći', 0)]\nMLP output [('Kontrola', 2), ('昌', 0), ('Sito', 0), ('崎', 0), ('пута', 0), ('prüfe', 0), ('阳', 0), ('乡', 0), ('mez', 0), ('bezeichneter', 0)]\nBlock output [('ery', 6), ('berga', 0), ('пута', 0), ('昌', 0), ('ো', 0), ('mez', 0), ('Apple', 0), ('阳', 0), ('Kontrola', 0), ('Ě', 0)]\n\nLayer 1: Decoded intermediate outputs\nAttention mechanism [('références', 1), ('chev', 0), ('isz', 0), ('rot', 0), ('embar', 0), ('Насе', 0), ('imedia', 0), ('ˇ', 0), ('older', 0), ('réseau', 0)]\nIntermediate residual stream [('ery', 20), ('print', 1), ('Außer', 1), ('wich', 0), ('mez', 0), ('mark', 0), ('ija', 0), ('dear', 0), ('impression', 0), ('lá', 0)]\nMLP output [('apers', 2), ('mez', 1), ('screen', 0), ('Screen', 0), ('tml', 0), ('ock', 0), ('wich', 0), ('len', 0), ('house', 0), ('ville', 0)]\nBlock output [('ery', 13), ('wich', 4), ('mark', 1), ('dear', 1), ('print', 0), ('mez', 0), ('len', 0), ('house', 0), ('multicol', 0), ('mez', 0)]\n\nLayer 2: Decoded intermediate outputs\nAttention mechanism [('aterra', 1), ('icture', 1), ('áj', 0), ('LCCN', 0), ('crew', 0), ('łow', 0), ('redirects', 0), ('ział', 0), ('hai', 0), ('aster', 0)]\nIntermediate residual stream [('wich', 8), ('ery', 7), ('stone', 1), ('mark', 1), ('mez', 1), ('dear', 0), ('Anto', 0), ('house', 0), ('print', 0), ('flex', 0)]\nMLP output [('ilde', 1), ('noise', 0), ('aris', 0), ('agens', 0), ('imit', 0), ('bil', 0), ('hus', 0), ('віт', 0), ('hab', 0), ('Ru', 0)]\nBlock output [('ery', 12), ('wich', 4), ('mez', 1), ('stone', 0), ('dear', 0), ('house', 0), ('apers', 0), ('eye', 0), ('jack', 0), ('unya', 0)]\n\nLayer 3: Decoded intermediate outputs\nAttention mechanism [('Gé', 1), ('Summary', 1), ('Хронологија', 0), ('nitt', 0), ('ikz', 0), ('atra', 0), ('Hou', 0), ('terior', 0), ('lacht', 0), ('WT', 0)]\nIntermediate residual stream [('ery', 3), ('house', 0), ('wich', 0), ('mez', 0), ('back', 0), ('arrow', 0), ('stone', 0), ('þ', 0), ('lass', 0), ('painted', 0)]\nMLP output [('iser', 2), ('itel', 1), ('uns', 1), ('Portug', 0), ('erson', 0), ('Gem', 0), ('iso', 0), ('own', 0), ('Sund', 0), ('jon', 0)]\nBlock output [('ery', 3), ('wich', 2), ('lass', 1), ('stone', 0), ('back', 0), ('house', 0), ('print', 0), ('ock', 0), ('lee', 0), ('itel', 0)]\n\nLayer 4: Decoded intermediate outputs\nAttention mechanism [('mi', 1), ('amba', 0), ('aterra', 0), ('oka', 0), ('Bay', 0), ('plug', 0), ('cible', 0), ('Sank', 0), ('Mi', 0), ('Ghost', 0)]\nIntermediate residual stream [('lass', 3), ('wich', 1), ('Bay', 0), ('Sar', 0), ('âtre', 0), ('gem', 0), ('istes', 0), ('narrow', 0), ('Sob', 0), ('shame', 0)]\nMLP output [('보', 5), ('vex', 2), ('arc', 2), ('sted', 1), ('alu', 1), ('ˇ', 1), ('ption', 0), ('--+', 0), ('site', 0), ('点', 0)]\nBlock output [('wich', 3), ('话', 2), ('bru', 0), ('цен', 0), ('lee', 0), ('ish', 0), ('iono', 0), ('ClassLoader', 0), ('話', 0), ('ченко', 0)]\n\nLayer 5: Decoded intermediate outputs\nAttention mechanism [('erei', 4), ('esterni', 4), ('�', 2), ('ethe', 1), ('败', 0), ('rok', 0), ('ipage', 0), ('ǐ', 0), ('itul', 0), ('ilon', 0)]\nIntermediate residual stream [('话', 3), ('wich', 1), ('bru', 1), ('話', 0), ('ченко', 0), ('lee', 0), ('ClassLoader', 0), ('ish', 0), ('цен', 0), ('erei', 0)]\nMLP output [('vel', 2), ('Pen', 0), ('tera', 0), ('м', 0), ('Publish', 0), ('pen', 0), ('ге', 0), ('äck', 0), ('ERR', 0), ('eign', 0)]\nBlock output [('wich', 2), ('ufen', 1), ('ifen', 1), ('erei', 1), ('thumb', 0), ('bru', 0), ('uso', 0), ('话', 0), ('eni', 0), ('hou', 0)]\n\nLayer 6: Decoded intermediate outputs\nAttention mechanism [('perty', 3), ('phon', 3), ('icode', 1), ('sigu', 0), ('öl', 0), ('ху', 0), ('raph', 0), ('ienne', 0), ('orum', 0), ('paste', 0)]\nIntermediate residual stream [('bru', 1), ('цен', 0), ('ioni', 0), ('ifen', 0), ('wich', 0), ('imation', 0), ('ufen', 0), ('uso', 0), ('erei', 0), ('laps', 0)]\nMLP output [('idel', 1), ('NR', 0), ('Mitchell', 0), ('rez', 0), ('cen', 0), ('asi', 0), ('ierra', 0), ('outer', 0), ('walls', 0), ('unix', 0)]\nBlock output [('wich', 2), ('цен', 0), ('erei', 0), ('Bent', 0), ('Apple', 0), ('iev', 0), ('hou', 0), ('ouvel', 0), ('imation', 0), ('pent', 0)]\n\nLayer 7: Decoded intermediate outputs\nAttention mechanism [('vba', 0), ('zar', 0), ('зво', 0), ('rijk', 0), ('visual', 0), ('Bit', 0), ('удо', 0), ('arina', 0), ('vern', 0), ('ponse', 0)]\nIntermediate residual stream [('wich', 1), ('pent', 0), ('Bent', 0), ('цен', 0), ('ouvel', 0), ('mail', 0), ('hou', 0), ('bru', 0), ('laps', 0), ('erei', 0)]\nMLP output [('aucoup', 2), ('Cong', 1), ('aña', 0), ('thick', 0), ('cí', 0), ('Academy', 0), ('eles', 0), ('cock', 0), ('defin', 0), ('ги', 0)]\nBlock output [('wich', 6), ('uso', 0), ('го', 0), ('ufen', 0), ('jpeg', 0), ('Apple', 0), ('цен', 0), ('apple', 0), ('hook', 0), ('Fuß', 0)]\n\nLayer 8: Decoded intermediate outputs\nAttention mechanism [('mor', 3), ('mor', 1), ('Cult', 1), ('wan', 1), ('ião', 0), ('scriptstyle', 0), ('ähr', 0), ('uta', 0), ('nou', 0), ('ymnas', 0)]\nIntermediate residual stream [('wich', 8), ('uso', 1), ('го', 0), ('clos', 0), ('pent', 0), ('imation', 0), ('ifen', 0), ('jpeg', 0), ('apple', 0), ('ufen', 0)]\nMLP output [('atura', 1), ('emet', 1), ('Sar', 0), ('Porto', 0), ('cho', 0), ('gov', 0), ('û', 0), ('信', 0), ('picture', 0), ('uni', 0)]\nBlock output [('jpeg', 1), ('wich', 0), ('emet', 0), ('ef', 0), ('onk', 0), ('gi', 0), ('Sar', 0), ('mail', 0), ('uso', 0), ('chor', 0)]\n\nLayer 9: Decoded intermediate outputs\nAttention mechanism [('ECK', 3), ('Jenkins', 1), ('andas', 1), ('Days', 1), ('ąż', 1), ('aden', 0), ('LIM', 0), ('éc', 0), ('black', 0), ('tec', 0)]\nIntermediate residual stream [('gi', 1), ('jpeg', 0), ('uso', 0), ('wich', 0), ('ocal', 0), ('onk', 0), ('bl', 0), ('emit', 0), ('Sar', 0), ('istre', 0)]\nMLP output [('attan', 1), ('Zar', 1), ('kl', 1), ('fatal', 1), ('full', 0), ('aden', 0), ('anta', 0), ('сси', 0), ('map', 0), ('optional', 0)]\nBlock output [('Zar', 1), ('flex', 1), ('anta', 0), ('ku', 0), ('Sar', 0), ('jpeg', 0), ('kre', 0), ('wich', 0), ('onk', 0), ('uso', 0)]\n\nLayer 10: Decoded intermediate outputs\nAttention mechanism [('È', 3), ('orum', 2), ('bay', 1), ('aget', 0), ('velocity', 0), ('Dean', 0), ('zew', 0), ('azar', 0), ('ALSE', 0), ('ername', 0)]\nIntermediate residual stream [('flex', 1), ('kre', 1), ('anta', 0), ('ku', 0), ('onk', 0), ('jpeg', 0), ('rare', 0), ('Sar', 0), ('uso', 0), ('Zar', 0)]\nMLP output [('favor', 2), ('mine', 1), ('White', 1), ('제', 1), ('Mine', 0), ('reb', 0), ('MY', 0), ('inkel', 0), ('ahr', 0), ('ccc', 0)]\nBlock output [('Sar', 3), ('flex', 0), ('ku', 0), ('uso', 0), ('anta', 0), ('onk', 0), ('endo', 0), ('wich', 0), ('Mill', 0), ('bl', 0)]\n\nLayer 11: Decoded intermediate outputs\nAttention mechanism [('merk', 1), ('tor', 0), ('revol', 0), ('Gh', 0), ('rak', 0), ('оте', 0), ('tact', 0), ('reu', 0), ('Kin', 0), ('etzt', 0)]\nIntermediate residual stream [('Sar', 2), ('ku', 1), ('anta', 0), ('ifen', 0), ('wich', 0), ('Mill', 0), ('onk', 0), ('uso', 0), ('flex', 0), ('kre', 0)]\nMLP output [('wor', 3), ('ster', 2), ('hmen', 0), ('oure', 0), ('bank', 0), ('interests', 0), ('ument', 0), ('imore', 0), ('сли', 0), ('neh', 0)]\nBlock output [('Sar', 3), ('wich', 1), ('cla', 0), ('Mill', 0), ('ku', 0), ('bl', 0), ('ifen', 0), ('patch', 0), ('Fun', 0), ('onk', 0)]\n\nLayer 12: Decoded intermediate outputs\nAttention mechanism [('avia', 1), ('育', 1), ('ggi', 1), ('Mail', 1), ('utto', 0), ('cust', 0), ('Bis', 0), ('appa', 0), ('ori', 0), ('FIX', 0)]\nIntermediate residual stream [('wich', 1), ('ku', 1), ('Sar', 1), ('cla', 1), ('bl', 0), ('ifen', 0), ('win', 0), ('Mac', 0), ('Mill', 0), ('meant', 0)]\nMLP output [('вид', 5), ('UMN', 0), ('ello', 0), ('born', 0), ('ennis', 0), ('bay', 0), ('Gün', 0), ('eed', 0), ('UTE', 0), ('гов', 0)]\nBlock output [('cla', 2), ('wich', 1), ('Sar', 1), ('ug', 0), ('ifen', 0), ('clause', 0), ('meant', 0), ('ku', 0), ('kre', 0), ('bl', 0)]\n\nLayer 13: Decoded intermediate outputs\nAttention mechanism [('oltre', 2), ('Autres', 0), ('Gé', 0), ('lands', 0), ('enberg', 0), ('mul', 0), ('repeating', 0), ('esp', 0), ('femin', 0), ('enburg', 0)]\nIntermediate residual stream [('wich', 2), ('ug', 1), ('cla', 1), ('Sar', 0), ('ifen', 0), ('meant', 0), ('ku', 0), ('clause', 0), ('lands', 0), ('endo', 0)]\nMLP output [('iech', 2), ('edia', 1), ('Mittel', 0), ('ору', 0), ('nick', 0), ('Herzog', 0), ('rikt', 0), ('TY', 0), ('esses', 0), ('Medi', 0)]\nBlock output [('wich', 4), ('Sar', 1), ('Kra', 1), ('kre', 0), ('wood', 0), ('Zar', 0), ('cla', 0), ('lands', 0), ('ug', 0), ('endo', 0)]\n\nLayer 14: Decoded intermediate outputs\nAttention mechanism [('Ej', 3), ('ct', 0), ('olf', 0), ('form', 0), ('oru', 0), ('libre', 0), ('pool', 0), ('Tem', 0), ('ctu', 0), ('No', 0)]\nIntermediate residual stream [('wich', 3), ('Sar', 1), ('kre', 1), ('endo', 0), ('Kra', 0), ('lands', 0), ('drum', 0), ('wood', 0), ('ele', 0), ('ug', 0)]\nMLP output [('col', 5), ('ery', 3), ('antry', 2), ('ish', 1), ('fach', 1), ('estr', 0), ('edy', 0), ('xsd', 0), ('esi', 0), ('Sky', 0)]\nBlock output [('wich', 12), ('ish', 1), ('UC', 0), ('per', 0), ('ku', 0), ('bid', 0), ('wid', 0), ('iful', 0), ('fin', 0), ('Sar', 0)]\n\nLayer 15: Decoded intermediate outputs\nAttention mechanism [('UC', 1), ('kre', 1), ('rh', 1), ('gla', 0), ('pod', 0), ('vote', 0), ('Proposition', 0), ('ències', 0), ('agua', 0), ('ymen', 0)]\nIntermediate residual stream [('wich', 10), ('UC', 3), ('ish', 1), ('kre', 0), ('fin', 0), ('ku', 0), ('wid', 0), ('lbl', 0), ('Haz', 0), ('ifen', 0)]\nMLP output [('Footnote', 1), ('тет', 1), ('Bedeut', 0), ('là', 0), ('års', 0), ('inks', 0), ('merk', 0), ('StackTrace', 0), ('imoine', 0), ('Einzel', 0)]\nBlock output [('wich', 19), ('UC', 2), ('ish', 1), ('hos', 0), ('spaces', 0), ('ery', 0), ('mail', 0), ('lbl', 0), ('Heinrich', 0), ('wid', 0)]\n\nLayer 16: Decoded intermediate outputs\nAttention mechanism [('eme', 1), ('Blues', 1), ('ols', 1), ('sem', 0), ('Bass', 0), ('sem', 0), ('Sem', 0), ('icol', 0), ('Chem', 0), ('blue', 0)]\nIntermediate residual stream [('wich', 24), ('UC', 3), ('hos', 0), ('ish', 0), ('spaces', 0), ('mail', 0), ('lbl', 0), ('eme', 0), ('lands', 0), ('fin', 0)]\nMLP output [('rug', 2), ('atus', 0), ('afka', 0), ('DA', 0), ('zes', 0), ('Ban', 0), ('vention', 0), ('ờ', 0), ('developer', 0), ('actor', 0)]\nBlock output [('wich', 10), ('UC', 3), ('ish', 1), ('hos', 1), ('ery', 0), ('spaces', 0), ('fin', 0), ('apple', 0), ('wood', 0), ('eme', 0)]\n\nLayer 17: Decoded intermediate outputs\nAttention mechanism [('♦', 2), ('Pho', 1), (':@', 1), ('пор', 0), ('ゼ', 0), ('кая', 0), ('références', 0), ('punkt', 0), ('testing', 0), ('Ses', 0)]\nIntermediate residual stream [('wich', 9), ('UC', 4), ('hos', 1), ('ish', 0), ('fin', 0), ('traffic', 0), ('wood', 0), ('col', 0), ('ery', 0), ('HL', 0)]\nMLP output [('estra', 1), ('meno', 0), ('emas', 0), ('back', 0), ('emb', 0), ('olean', 0), ('ainer', 0), ('UNION', 0), ('dot', 0), ('̍', 0)]\nBlock output [('wich', 14), ('UC', 6), ('ish', 5), ('ery', 2), ('fin', 0), ('hos', 0), ('azionale', 0), ('Ce', 0), ('dot', 0), ('HL', 0)]\n\nLayer 18: Decoded intermediate outputs\nAttention mechanism [('outer', 1), ('bres', 0), ('ombres', 0), ('trans', 0), ('res', 0), ('mez', 0), ('obi', 0), ('aca', 0), ('wand', 0), ('fred', 0)]\nIntermediate residual stream [('wich', 10), ('UC', 6), ('ish', 4), ('ery', 1), ('dot', 0), ('hos', 0), ('HL', 0), ('azionale', 0), ('kre', 0), ('fin', 0)]\nMLP output [('awa', 4), ('äs', 2), ('ery', 1), ('Bes', 1), ('Hell', 1), ('ску', 1), ('vice', 1), ('еде', 0), ('estaven', 0), ('ění', 0)]\nBlock output [('wich', 19), ('ery', 10), ('ish', 7), ('UC', 6), ('dot', 0), ('eme', 0), ('eyes', 0), ('Bean', 0), ('ovis', 0), ('beans', 0)]\n\nLayer 19: Decoded intermediate outputs\nAttention mechanism [('od', 1), ('Person', 0), ('plex', 0), ('otrop', 0), ('Ress', 0), ('fg', 0), ('良', 0), ('dom', 0), ('oracle', 0), ('quer', 0)]\nIntermediate residual stream [('wich', 16), ('ery', 7), ('UC', 6), ('ish', 6), ('dot', 1), ('eme', 0), ('beans', 0), ('Ce', 0), ('eyes', 0), ('Bean', 0)]\nMLP output [('thumb', 18), ('buch', 2), ('erm', 0), ('lon', 0), ('Tamb', 0), ('zie', 0), ('оло', 0), ('est', 0), ('alet', 0), ('lik', 0)]\nBlock output [('ish', 29), ('wich', 15), ('ery', 5), ('UC', 3), ('Ce', 0), ('beans', 0), ('house', 0), ('isen', 0), ('ce', 0), ('Bean', 0)]\n\nLayer 20: Decoded intermediate outputs\nAttention mechanism [('color', 75), ('color', 11), ('Color', 11), ('Color', 0), ('colors', 0), ('colored', 0), ('colors', 0), ('Colors', 0), ('colour', 0), ('色', 0)]\nIntermediate residual stream [('ish', 28), ('wich', 16), ('ery', 4), ('UC', 3), ('Ce', 0), ('light', 0), ('beans', 0), ('house', 0), ('isen', 0), ('transparent', 0)]\nMLP output [('gre', 2), ('jul', 0), ('vá', 0), ('green', 0), ('Green', 0), ('town', 0), ('Пу', 0), ('proper', 0), ('bel', 0), ('Ready', 0)]\nBlock output [('ish', 39), ('wich', 7), ('UC', 2), ('ery', 2), ('spaces', 0), ('house', 0), ('Ce', 0), ('isen', 0), ('transparent', 0), ('ishes', 0)]\n\nLayer 21: Decoded intermediate outputs\nAttention mechanism [('color', 2), ('odon', 1), ('uba', 0), ('carbon', 0), ('Color', 0), ('arda', 0), ('forces', 0), ('obox', 0), ('aft', 0), ('wagen', 0)]\nIntermediate residual stream [('ish', 37), ('wich', 7), ('UC', 2), ('ery', 1), ('spaces', 0), ('house', 0), ('Ce', 0), ('light', 0), ('isen', 0), ('alg', 0)]\nMLP output [('len', 1), ('method', 1), ('Reset', 0), ('typeof', 0), ('Graph', 0), ('Graph', 0), ('üs', 0), ('�', 0), ('team', 0), ('law', 0)]\nBlock output [('ish', 30), ('wich', 4), ('ery', 2), ('UC', 1), ('spaces', 0), ('jub', 0), ('field', 0), ('Bean', 0), ('light', 0), ('ough', 0)]\n\nLayer 22: Decoded intermediate outputs\nAttention mechanism [('ücke', 0), ('�', 0), ('enes', 0), ('pol', 0), ('red', 0), ('기', 0), ('orf', 0), ('dorf', 0), ('Route', 0), ('ß', 0)]\nIntermediate residual stream [('ish', 25), ('wich', 3), ('ery', 2), ('UC', 1), ('esi', 0), ('field', 0), ('spaces', 0), ('gr', 0), ('ces', 0), ('fin', 0)]\nMLP output [('клад', 1), ('atif', 0), ('house', 0), ('gets', 0), ('dur', 0), ('efe', 0), ('dom', 0), ('hash', 0), ('поль', 0), ('ĭ', 0)]\nBlock output [('ish', 16), ('wich', 2), ('house', 2), ('gr', 1), ('esi', 1), ('ery', 0), ('spaces', 0), ('alg', 0), ('UC', 0), ('ius', 0)]\n\nLayer 23: Decoded intermediate outputs\nAttention mechanism [('сов', 0), ('ров', 0), ('Попис', 0), ('aria', 0), ('agen', 0), ('maz', 0), ('hina', 0), ('Хронологија', 0), ('azar', 0), ('unda', 0)]\nIntermediate residual stream [('ish', 13), ('wich', 2), ('house', 1), ('gr', 1), ('ery', 1), ('spaces', 0), ('esi', 0), ('alg', 0), ('ough', 0), ('UC', 0)]\nMLP output [('kw', 8), ('ire', 1), ('sun', 1), ('Plant', 0), ('Otto', 0), ('бір', 0), ('lit', 0), ('ween', 0), ('iero', 0), ('aggio', 0)]\nBlock output [('ish', 14), ('house', 4), ('wich', 1), ('spaces', 1), ('lit', 1), ('light', 0), ('esi', 0), ('dawn', 0), ('alg', 0), ('gr', 0)]\n\nLayer 24: Decoded intermediate outputs\nAttention mechanism [('color', 57), ('color', 10), ('colors', 8), ('Color', 4), ('colour', 2), ('Color', 1), ('colours', 1), ('colored', 1), ('Colors', 1), ('colors', 0)]\nIntermediate residual stream [('ish', 16), ('house', 3), ('spaces', 1), ('lit', 1), ('wich', 1), ('light', 1), ('alg', 0), ('esi', 0), ('gr', 0), ('dawn', 0)]\nMLP output [('host', 1), ('esi', 1), ('ur', 1), ('lá', 1), ('Gr', 0), ('host', 0), ('ilon', 0), ('herit', 0), ('idge', 0), ('perman', 0)]\nBlock output [('ish', 11), ('house', 8), ('esi', 4), ('lit', 2), ('ies', 1), ('gr', 0), ('spaces', 0), ('back', 0), ('wich', 0), ('ough', 0)]\n\nLayer 25: Decoded intermediate outputs\nAttention mechanism [('scal', 1), ('uen', 1), ('omp', 0), ('Sans', 0), ('берг', 0), ('curs', 0), ('mun', 0), ('hath', 0), ('oct', 0), ('dic', 0)]\nIntermediate residual stream [('ish', 10), ('house', 6), ('esi', 4), ('lit', 1), ('ies', 1), ('gr', 1), ('spaces', 0), ('wich', 0), ('ough', 0), ('ery', 0)]\nMLP output [('gerufen', 1), ('/~', 1), ('Dynamic', 0), ('ič', 0), ('eerd', 0), ('ös', 0), ('Convert', 0), ('schen', 0), ('rvm', 0), ('ány', 0)]\nBlock output [('ish', 5), ('house', 4), ('esi', 4), ('ery', 1), ('lit', 1), ('light', 1), ('wich', 1), ('gr', 0), ('oshi', 0), ('bean', 0)]\n\nLayer 26: Decoded intermediate outputs\nAttention mechanism [('only', 1), ('only', 1), ('Committee', 1), ('dt', 0), ('Ry', 0), ('stone', 0), ('Hou', 0), ('pc', 0), ('asa', 0), ('Bere', 0)]\nIntermediate residual stream [('ish', 3), ('esi', 3), ('house', 2), ('ery', 2), ('light', 1), ('lit', 0), ('bean', 0), ('las', 0), ('wich', 0), ('Dragon', 0)]\nMLP output [('Lib', 1), ('capital', 1), ('par', 0), ('em', 0), ('d', 0), ('Ax', 0), ('inha', 0), ('パ', 0), ('O', 0), ('пар', 0)]\nBlock output [('ish', 4), ('esi', 2), ('house', 2), ('light', 1), ('las', 1), ('leaf', 0), ('thumb', 0), ('hell', 0), ('alg', 0), ('gr', 0)]\n\nLayer 27: Decoded intermediate outputs\nAttention mechanism [('undle', 2), ('że', 1), ('acia', 1), ('j', 1), ('aben', 0), ('mod', 0), ('Variable', 0), ('Braun', 0), ('ugust', 0), ('Ram', 0)]\nIntermediate residual stream [('ish', 4), ('esi', 3), ('house', 2), ('light', 1), ('las', 0), ('ery', 0), ('thumb', 0), ('leaf', 0), ('alg', 0), ('flash', 0)]\nMLP output [('eken', 0), ('Jahrh', 0), ('cub', 0), ('Оте', 0), ('bbi', 0), ('ela', 0), ('Mys', 0), ('Rat', 0), ('élé', 0), ('прави', 0)]\nBlock output [('ish', 5), ('esi', 2), ('las', 2), ('house', 0), ('ois', 0), ('flash', 0), ('light', 0), ('back', 0), ('gef', 0), ('bott', 0)]\n\nLayer 28: Decoded intermediate outputs\nAttention mechanism [('P', 22), ('S', 5), ('East', 2), ('West', 1), ('Dé', 0), ('Don', 0), ('DA', 0), ('Dic', 0), ('DA', 0), ('DM', 0)]\nIntermediate residual stream [('ish', 5), ('las', 2), ('esi', 2), ('flash', 0), ('house', 0), ('light', 0), ('ois', 0), ('back', 0), ('Pages', 0), ('Dragon', 0)]\nMLP output [('istrzost', 2), ('hill', 1), ('Hill', 1), ('�', 0), ('religion', 0), ('elle', 0), ('grammar', 0), ('umerate', 0), ('essa', 0), ('ince', 0)]\nBlock output [('ish', 4), ('esi', 1), ('gr', 1), ('las', 1), ('Dragon', 0), ('back', 0), ('house', 0), ('light', 0), ('fluid', 0), ('flash', 0)]\n\nLayer 29: Decoded intermediate outputs\nAttention mechanism [('dry', 1), ('lish', 0), ('8', 0), ('Press', 0), ('uen', 0), ('Las', 0), ('press', 0), ('gr', 0), ('meister', 0), ('ribu', 0)]\nIntermediate residual stream [('ish', 3), ('gr', 1), ('las', 1), ('esi', 1), ('back', 0), ('light', 0), ('fluid', 0), ('flash', 0), ('Dragon', 0), ('house', 0)]\nMLP output [('ide', 58), ('Ide', 29), ('IDE', 4), ('ide', 4), ('ideas', 1), ('IDE', 0), ('idea', 0), ('иде', 0), ('ideal', 0), ('l', 0)]\nBlock output [('ideas', 6), ('ide', 6), ('ish', 3), ('las', 3), ('ies', 1), ('idea', 1), ('IDE', 1), ('Ide', 1), ('IDE', 0), ('gr', 0)]\n\nLayer 30: Decoded intermediate outputs\nAttention mechanism [('[', 0), ('...', 0), ('buy', 0), ('...', 0), ('`', 0), ('mens', 0), ('por', 0), ('Љ', 0), ('Por', 0), ('aph', 0)]\nIntermediate residual stream [('ideas', 6), ('ide', 5), ('las', 2), ('ish', 2), ('ies', 1), ('Ide', 1), ('idea', 1), ('IDE', 1), ('IDE', 0), ('Las', 0)]\nMLP output [('T', 97), ('m', 0), ('V', 0), ('tr', 0), ('\\n', 0), ('(', 0), ('s', 0), ('H', 0), ('ch', 0), ('e', 0)]\nBlock output [('ideas', 8), ('las', 5), ('ish', 4), ('ide', 4), ('-', 3), ('d', 2), ('g', 2), ('\\n', 2), ('.', 1), ('ies', 1)]\n\nLayer 31: Decoded intermediate outputs\nAttention mechanism [('...', 1), ('...', 1), ('Sym', 1), ('geomet', 0), ('b', 0), ('ones', 0), ('seconds', 0), ('abeth', 0), ('uch', 0), ('lob', 0)]\nIntermediate residual stream [('las', 9), ('ideas', 6), ('-', 5), ('.', 3), ('d', 2), ('\\n', 2), ('g', 2), ('ish', 2), ('ide', 2), (',', 1)]\nMLP output [('ideas', 2), ('thoughts', 2), ('elli', 2), ('medal', 2), ('hill', 1), ('eggs', 1), ('curl', 1), ('hills', 0), ('clouds', 0), ('epo', 0)]\nBlock output [('ideas', 72), ('d', 1), ('is', 1), ('-', 0), ('.', 0), ('and', 0), ('g', 0), ('las', 0), ('eggs', 0), (',', 0)]\n\n","output_type":"stream"}]}]}