{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9317887,"sourceType":"datasetVersion","datasetId":5643951},{"sourceId":108099,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":90542,"modelId":114758}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<h1>Notebook for LLM Workshop</h1>\n<h2>Tommi Buder-Gr√∂ndahl</h2>\n<h3>Improvements by Andrea Ferreira</h3>\n<h3>Set2024</h3>\n</center>","metadata":{}},{"cell_type":"markdown","source":"## Part 1. Examining and fine-tuning BERT\n\nBased on a tutorial from [Kaggle](https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation)\n","metadata":{}},{"cell_type":"code","source":"# Import Python libraries\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:15:36.027603Z","iopub.execute_input":"2024-09-09T14:15:36.027886Z","iopub.status.idle":"2024-09-09T14:15:40.744294Z","shell.execute_reply.started":"2024-09-09T14:15:36.027854Z","shell.execute_reply":"2024-09-09T14:15:40.743314Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Getting BERT and its tokenizer","metadata":{}},{"cell_type":"code","source":"# Import the Transformers library and download BERT\n\nfrom transformers import AutoModel, BertTokenizerFast, AdamW\n\nbert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:22:47.181840Z","iopub.execute_input":"2024-09-09T14:22:47.182188Z","iopub.status.idle":"2024-09-09T14:22:47.186239Z","shell.execute_reply.started":"2024-09-09T14:22:47.182154Z","shell.execute_reply":"2024-09-09T14:22:47.185330Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"bert","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:22:59.699196Z","iopub.execute_input":"2024-09-09T14:22:59.699640Z","iopub.status.idle":"2024-09-09T14:22:59.708304Z","shell.execute_reply.started":"2024-09-09T14:22:59.699595Z","shell.execute_reply":"2024-09-09T14:22:59.707388Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:23:04.297024Z","iopub.execute_input":"2024-09-09T14:23:04.297880Z","iopub.status.idle":"2024-09-09T14:23:04.304211Z","shell.execute_reply.started":"2024-09-09T14:23:04.297836Z","shell.execute_reply":"2024-09-09T14:23:04.303282Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Defining the BERT-based sentence classifier architecture","metadata":{}},{"cell_type":"code","source":"##### ANDREA \n\nclass BERT_Arch(nn.Module):\n    def __init__(self, bert):\n        super(BERT_Arch, self).__init__()\n        \n        # Assign the pre-trained BERT model to the class\n        self.bert = bert\n        \n        # First fully connected (dense) layer with input size of 768 (BERT hidden state size) \n        # and output size of 512\n        self.fc1 = nn.Linear(768, 512)\n        \n        # Second fully connected (dense) layer with output size of 2 (for binary classification)\n        self.fc2 = nn.Linear(512, 2)\n        \n        # Dropout layer with a 10% dropout rate to prevent overfitting\n        self.dropout = nn.Dropout(0.1)\n        \n        # ReLU activation function (Rectified Linear Unit) to introduce non-linearity\n        self.relu = nn.ReLU()\n        \n        # LogSoftmax function to convert the output to log probabilities\n        # across the two target classes (for binary classification)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, sent_id, mask):\n        # Pass the inputs to the BERT model and get the output for the [CLS] token\n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n        \n        # Pass the [CLS] output through the first fully connected layer (fc1)\n        x = self.fc1(cls_hs)\n        \n        # Apply the ReLU activation function\n        x = self.relu(x)\n        \n        # Apply dropout to reduce overfitting\n        x = self.dropout(x)\n        \n        # Pass the result through the second fully connected layer (fc2) for the final output\n        x = self.fc2(x)\n        \n        # Apply LogSoftmax to get log probabilities for each class\n        return self.softmax(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:23:52.832814Z","iopub.execute_input":"2024-09-09T14:23:52.833421Z","iopub.status.idle":"2024-09-09T14:23:52.842459Z","shell.execute_reply.started":"2024-09-09T14:23:52.833376Z","shell.execute_reply":"2024-09-09T14:23:52.841469Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = BERT_Arch(bert)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:24:25.941310Z","iopub.execute_input":"2024-09-09T14:24:25.941723Z","iopub.status.idle":"2024-09-09T14:24:25.952665Z","shell.execute_reply.started":"2024-09-09T14:24:25.941683Z","shell.execute_reply":"2024-09-09T14:24:25.951827Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Put model to GPU if available\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T15:04:01.147157Z","iopub.execute_input":"2024-09-09T15:04:01.148064Z","iopub.status.idle":"2024-09-09T15:04:01.552549Z","shell.execute_reply.started":"2024-09-09T15:04:01.148021Z","shell.execute_reply":"2024-09-09T15:04:01.551730Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:28:24.066796Z","iopub.execute_input":"2024-09-04T15:28:24.067126Z","iopub.status.idle":"2024-09-04T15:28:24.074921Z","shell.execute_reply.started":"2024-09-04T15:28:24.067091Z","shell.execute_reply":"2024-09-04T15:28:24.074030Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BERT_Arch(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (relu): ReLU()\n  (fc1): Linear(in_features=768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (softmax): LogSoftmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Getting the dataset","metadata":{}},{"cell_type":"code","source":"# use pre-labeled spam detection dataset\ndf = pd.read_csv(\"/kaggle/input/spamdata-v2/spamdata_v2.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:24:59.979532Z","iopub.execute_input":"2024-09-09T14:24:59.980490Z","iopub.status.idle":"2024-09-09T14:25:00.045404Z","shell.execute_reply.started":"2024-09-09T14:24:59.980439Z","shell.execute_reply":"2024-09-09T14:25:00.044557Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"      label                                               text\n0         0  Go until jurong point, crazy.. Available only ...\n1         0                      Ok lar... Joking wif u oni...\n2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n3         0  U dun say so early hor... U c already then say...\n4         0  Nah I don't think he goes to usf, he lives aro...\n...     ...                                                ...\n5567      1  This is the 2nd time we have tried 2 contact u...\n5568      0              Will √å_ b going to esplanade fr home?\n5569      0  Pity, * was in mood for that. So...any other s...\n5570      0  The guy did some bitching but I acted like i'd...\n5571      0                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>1</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>0</td>\n      <td>Will √å_ b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>0</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>0</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>0</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows √ó 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"##### ANDREA\n\n# Print the number of occurrences for each label\nlabel_counts = df['label'].value_counts()\n\nprint(\"Number of occurrences for each label:\")\nprint(label_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:25:41.974629Z","iopub.execute_input":"2024-09-09T14:25:41.975360Z","iopub.status.idle":"2024-09-09T14:25:41.990539Z","shell.execute_reply.started":"2024-09-09T14:25:41.975320Z","shell.execute_reply":"2024-09-09T14:25:41.989587Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Number of occurrences for each label:\nlabel\n0    4825\n1     747\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"##### ANDREA\n\n# Plot bar chart for label distribution\n\nimport matplotlib.pyplot as plt\n\nlabel_counts = df['label'].value_counts()\n\ncolors = ['#1f77b4', '#ff7f0e']  # Blue for 0, Orange for 1\n\nlabel_counts.plot(kind='bar', color=colors, edgecolor='black', width=0.8)\n\nplt.title('Distribution of Binary Labels')\nplt.xlabel('Labels')\nplt.ylabel('Count')\n\n# Rotate x-axis labels to horizontal\nplt.xticks(rotation=0)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T11:15:04.735196Z","iopub.execute_input":"2024-09-09T11:15:04.735844Z","iopub.status.idle":"2024-09-09T11:15:05.044659Z","shell.execute_reply.started":"2024-09-09T11:15:04.735808Z","shell.execute_reply":"2024-09-09T11:15:05.043576Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"##### ANDREA\n\n# Balance the dataset\n\nimport pandas as pd\nfrom sklearn.utils import resample\n\n# Split the dataset into two parts based on labels\ndf_majority = df[df['label'] == 0]  # Majority class (label 0)\ndf_minority = df[df['label'] == 1]  # Minority class (label 1)\n\n##### Option Undersampling\n\n### Random Undersampling (reduce majority class to the size of the minority class)\ndf_majority_downsampled = resample(df_majority, \n                                   replace=False,    # sample without replacement\n                                   n_samples=len(df_minority),  # match minority class\n                                   random_state=42)  # reproducible results\n\n# Combine minority class with downsampled majority class\ndf_balanced_undersample = pd.concat([df_majority_downsampled, df_minority])\n\n# Print the number of occurrences after undersampling\nprint(\"After undersampling:\")\nprint(df_balanced_undersample['label'].value_counts())\n\n\n##### Option Oversampling\n\n### Random Oversampling (increase minority class to the size of the majority class)\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=len(df_majority),  # match majority class\n                                 random_state=42)  # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_balanced_oversample = pd.concat([df_majority, df_minority_upsampled])\n\n# Print the number of occurrences after oversampling\nprint(\"After oversampling:\")\nprint(df_balanced_oversample['label'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:32:00.009883Z","iopub.execute_input":"2024-09-09T14:32:00.010470Z","iopub.status.idle":"2024-09-09T14:32:00.027697Z","shell.execute_reply.started":"2024-09-09T14:32:00.010431Z","shell.execute_reply":"2024-09-09T14:32:00.026827Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"After undersampling:\nlabel\n0    747\n1    747\nName: count, dtype: int64\nAfter oversampling:\nlabel\n0    4825\n1    4825\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df_balanced_undersample  # uncheck this comment for undersampling\n# df = df_balanced_oversample  # uncheck this comment for oversampling","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:32:07.811092Z","iopub.execute_input":"2024-09-09T14:32:07.811965Z","iopub.status.idle":"2024-09-09T14:32:07.815937Z","shell.execute_reply.started":"2024-09-09T14:32:07.811923Z","shell.execute_reply":"2024-09-09T14:32:07.814947Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"##### ANDREA\n\nfrom sklearn.model_selection import train_test_split\n\n# Function to split and tokenize datasets\ndef tokenize_texts(texts, tokenizer, max_length=25):\n    return tokenizer.batch_encode_plus(\n        texts.tolist(),\n        max_length=max_length,\n        padding='max_length',\n        truncation=True\n    )\n\n# Split the dataset into train, validation, and test sets\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(\n    df['text'], df['label'], \n    random_state=2024, \n    test_size=0.3, \n    stratify=df['label']\n)\n\nval_text, test_text, val_labels, test_labels = train_test_split(\n    temp_text, temp_labels, \n    random_state=2024, \n    test_size=0.5, \n    stratify=temp_labels\n)\n\n# Tokenize and encode sequences\ntokens_train = tokenize_texts(train_text, tokenizer)\ntokens_val = tokenize_texts(val_text, tokenizer)\ntokens_test = tokenize_texts(test_text, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:40:01.702387Z","iopub.execute_input":"2024-09-09T14:40:01.703325Z","iopub.status.idle":"2024-09-09T14:40:01.797984Z","shell.execute_reply.started":"2024-09-09T14:40:01.703279Z","shell.execute_reply":"2024-09-09T14:40:01.797221Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# convert lists to tensors\n\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:41:06.827893Z","iopub.execute_input":"2024-09-09T14:41:06.828245Z","iopub.status.idle":"2024-09-09T14:41:06.868334Z","shell.execute_reply.started":"2024-09-09T14:41:06.828211Z","shell.execute_reply":"2024-09-09T14:41:06.867482Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# train_mask is used for ignoring input padding in attention\n\nprint(tokenizer.decode(train_seq[0]))\nprint(train_mask[0])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-09T14:41:10.515146Z","iopub.execute_input":"2024-09-09T14:41:10.515543Z","iopub.status.idle":"2024-09-09T14:41:10.526770Z","shell.execute_reply.started":"2024-09-09T14:41:10.515507Z","shell.execute_reply":"2024-09-09T14:41:10.525558Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[CLS] gent! we are trying to contact you. last weekends draw shows that you won a a¬£1000 prize [SEP]\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training the classifier","metadata":{}},{"cell_type":"code","source":"# define a batch size\nbatch_size = 32\n\n# wrap tensors for training data\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for randomly sampling data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for training data\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors for validation data\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for randomly sampling data during validation\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:42:57.699706Z","iopub.execute_input":"2024-09-09T14:42:57.700069Z","iopub.status.idle":"2024-09-09T14:42:57.706556Z","shell.execute_reply.started":"2024-09-09T14:42:57.700036Z","shell.execute_reply":"2024-09-09T14:42:57.705547Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# freeze all BERT parameters (training only linear layers outside of BERT)\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:07.010613Z","iopub.execute_input":"2024-09-09T14:43:07.010978Z","iopub.status.idle":"2024-09-09T14:43:07.016674Z","shell.execute_reply.started":"2024-09-09T14:43:07.010942Z","shell.execute_reply":"2024-09-09T14:43:07.015688Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# define the optimizer\noptimizer = AdamW(model.parameters(),lr = 1e-5) \n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n\n# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-09-09T15:04:11.232143Z","iopub.execute_input":"2024-09-09T15:04:11.232547Z","iopub.status.idle":"2024-09-09T15:04:11.243626Z","shell.execute_reply.started":"2024-09-09T15:04:11.232507Z","shell.execute_reply":"2024-09-09T15:04:11.242700Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"##### ANDREA\n\n# Function to train the model\ndef train():\n    # Set the model to training mode (this activates Dropout layers)\n    model.train()\n\n    # Initialize total loss and accuracy for tracking progress\n    total_loss, total_accuracy = 0, 0\n    total_preds = []  # To store model predictions\n\n    # Loop through each batch of data in the training set\n    for step, batch in enumerate(train_dataloader):\n        \n        # Print progress every 50 batches\n        if step % 50 == 0 and step != 0:\n            print(f'  Batch {step} of {len(train_dataloader)}.')\n\n        # Move the batch to the device (GPU or CPU)\n        batch = [r.to(device) for r in batch]\n        sent_id, mask, labels = batch\n\n        # Reset any previously calculated gradients\n        model.zero_grad()\n\n        # Make predictions for this batch of data\n        preds = model(sent_id, mask)\n\n        # Calculate the loss (difference between actual and predicted labels)\n        loss = cross_entropy(preds, labels)\n\n        # Add this batch's loss to the total loss\n        total_loss += loss.item()\n\n        # Backpropagation: Calculate the gradients\n        loss.backward()\n\n        # Clip gradients to prevent them from getting too large (helps stability)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update the model parameters based on the calculated gradients\n        optimizer.step()\n\n        # Move predictions to the CPU and store them\n        preds = preds.detach().cpu().numpy()\n        total_preds.append(preds)\n\n        # Calculate accuracy for this batch\n        preds_flat = np.argmax(preds, axis=1)\n        labels_flat = labels.cpu().numpy()\n        batch_accuracy = np.sum(preds_flat == labels_flat) / len(labels_flat)\n        total_accuracy += batch_accuracy\n\n    # Calculate the average loss and accuracy for the entire epoch\n    avg_loss = total_loss / len(train_dataloader)\n    avg_accuracy = total_accuracy / len(train_dataloader) ##*** IT WAS MISSING\n\n    # Concatenate all predictions into one array\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    # Return the average loss, average accuracy, and all the predictions\n    return avg_loss, avg_accuracy, total_preds\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:56:30.733088Z","iopub.execute_input":"2024-09-09T14:56:30.733853Z","iopub.status.idle":"2024-09-09T14:56:30.743822Z","shell.execute_reply.started":"2024-09-09T14:56:30.733812Z","shell.execute_reply":"2024-09-09T14:56:30.742954Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"##### ANDREA\n    \n# Function for evaluating the model\ndef evaluate():\n    \n    print(\"\\nEvaluating...\")\n  \n    # Set the model to evaluation mode (deactivates Dropout layers)\n    model.eval()\n\n    # Initialize total loss and accuracy\n    total_loss, total_accuracy = 0, 0\n    \n    # Empty list to save the model predictions\n    total_preds = []\n\n    # Iterate over batches in the validation set\n    for step, batch in enumerate(val_dataloader):\n        \n        # Progress update every 50 batches\n        if step % 50 == 0 and step != 0:\n            print(f'  Batch {step} of {len(val_dataloader)}.')\n\n        # Move the batch to the device (GPU or CPU)\n        batch = [t.to(device) for t in batch]\n        sent_id, mask, labels = batch\n\n        # Deactivate autograd (no gradient calculation)\n        with torch.no_grad():\n            \n            # Model predictions for the current batch\n            preds = model(sent_id, mask)\n\n            # Compute the validation loss\n            loss = cross_entropy(preds, labels)\n\n            # Add the batch loss to the total loss\n            total_loss += loss.item()\n\n            # Move predictions to the CPU\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n\n            # Calculate accuracy for this batch\n            preds_flat = np.argmax(preds, axis=1)\n            labels_flat = labels.cpu().numpy()\n            batch_accuracy = np.sum(preds_flat == labels_flat) / len(labels_flat)\n            total_accuracy += batch_accuracy\n\n    # Compute the average loss and accuracy for the entire validation set\n    avg_loss = total_loss / len(val_dataloader)\n    avg_accuracy = total_accuracy / len(val_dataloader)\n\n    # Concatenate all predictions into one array\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    # Return the average loss, average accuracy, and predictions\n    return avg_loss, avg_accuracy, total_preds\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:59:14.555308Z","iopub.execute_input":"2024-09-09T14:59:14.555973Z","iopub.status.idle":"2024-09-09T14:59:14.565494Z","shell.execute_reply.started":"2024-09-09T14:59:14.555933Z","shell.execute_reply":"2024-09-09T14:59:14.564527Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"##### ANDREA\n\n# Set initial best validation loss to infinite\nbest_valid_loss = float('inf')\n\n# Lists to store training and validation losses and accuracies for each epoch\ntrain_losses = []\nvalid_losses = []\ntrain_accuracies = []\nvalid_accuracies = []\n\n# Number of epochs to train the model\nfor epoch in range(epochs):\n    \n    print(f'\\n Epoch {epoch + 1} / {epochs}')\n    \n    # Train the model and get the training loss and accuracy\n    train_loss, train_accuracy, _ = train()\n    \n    # Evaluate the model and get the validation loss and accuracy\n    valid_loss, valid_accuracy, _ = evaluate()\n    \n    # Save the best model if the validation loss decreases\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), '/kaggle/working/saved_weights.pt')\n    \n    # Append training and validation loss and accuracy to their respective lists\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accuracies.append(train_accuracy)\n    valid_accuracies.append(valid_accuracy)\n    \n    # Print training and validation results for this epoch\n    print(f'\\nTraining Loss: {train_loss:.3f} | Training Accuracy: {train_accuracy:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f} | Validation Accuracy: {valid_accuracy:.3f}')\n\n# Load the best model (with the lowest validation loss) for testing\npath = '/kaggle/working/saved_weights.pt'\nmodel.load_state_dict(torch.load(path))\n\n# Evaluate the model on the test set\nwith torch.no_grad():\n    preds = model(test_seq.to(device), test_mask.to(device))\n    preds = preds.detach().cpu().numpy()\n\n# Convert the predictions to label indices\npreds = np.argmax(preds, axis=1)\n\n# Print the classification report for the test set\nprint(classification_report(test_y, preds))\n\n# Free GPU memory (optional)\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T15:05:36.705240Z","iopub.execute_input":"2024-09-09T15:05:36.705912Z","iopub.status.idle":"2024-09-09T15:06:12.531685Z","shell.execute_reply.started":"2024-09-09T15:05:36.705874Z","shell.execute_reply":"2024-09-09T15:06:12.530743Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n\nEvaluating...\n\nTraining Loss: 0.592 | Training Accuracy: 0.746\nValidation Loss: 0.579 | Validation Accuracy: 0.826\n\n Epoch 2 / 10\n\nEvaluating...\n\nTraining Loss: 0.586 | Training Accuracy: 0.775\nValidation Loss: 0.571 | Validation Accuracy: 0.830\n\n Epoch 3 / 10\n\nEvaluating...\n\nTraining Loss: 0.574 | Training Accuracy: 0.764\nValidation Loss: 0.562 | Validation Accuracy: 0.839\n\n Epoch 4 / 10\n\nEvaluating...\n\nTraining Loss: 0.569 | Training Accuracy: 0.792\nValidation Loss: 0.555 | Validation Accuracy: 0.830\n\n Epoch 5 / 10\n\nEvaluating...\n\nTraining Loss: 0.561 | Training Accuracy: 0.778\nValidation Loss: 0.547 | Validation Accuracy: 0.839\n\n Epoch 6 / 10\n\nEvaluating...\n\nTraining Loss: 0.549 | Training Accuracy: 0.781\nValidation Loss: 0.539 | Validation Accuracy: 0.844\n\n Epoch 7 / 10\n\nEvaluating...\n\nTraining Loss: 0.546 | Training Accuracy: 0.796\nValidation Loss: 0.532 | Validation Accuracy: 0.839\n\n Epoch 8 / 10\n\nEvaluating...\n\nTraining Loss: 0.545 | Training Accuracy: 0.792\nValidation Loss: 0.524 | Validation Accuracy: 0.848\n\n Epoch 9 / 10\n\nEvaluating...\n\nTraining Loss: 0.534 | Training Accuracy: 0.814\nValidation Loss: 0.519 | Validation Accuracy: 0.826\n\n Epoch 10 / 10\n\nEvaluating...\n\nTraining Loss: 0.532 | Training Accuracy: 0.789\nValidation Loss: 0.510 | Validation Accuracy: 0.853\n              precision    recall  f1-score   support\n\n           0       0.88      0.80      0.84       113\n           1       0.81      0.89      0.85       112\n\n    accuracy                           0.84       225\n   macro avg       0.85      0.84      0.84       225\nweighted avg       0.85      0.84      0.84       225\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## loading saved model\n\nimport torch\n\nsaved_model_path = '/kaggle/input/saved_weights.pt/transformers/default/1/saved_weights.pt'\nmodel = torch.load(saved_model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T15:07:56.929388Z","iopub.execute_input":"2024-09-09T15:07:56.929969Z","iopub.status.idle":"2024-09-09T15:07:56.933693Z","shell.execute_reply.started":"2024-09-09T15:07:56.929930Z","shell.execute_reply":"2024-09-09T15:07:56.932787Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Part 2. Sentence similarities with SentenceBERT\n\nBased on [SentenceBERT documentation](https://sbert.net/)","metadata":{}},{"cell_type":"code","source":"# !pip install sentence-transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T11:27:49.185367Z","iopub.execute_input":"2024-09-09T11:27:49.185769Z","iopub.status.idle":"2024-09-09T11:27:49.190559Z","shell.execute_reply.started":"2024-09-09T11:27:49.185732Z","shell.execute_reply":"2024-09-09T11:27:49.189400Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T11:27:52.922839Z","iopub.execute_input":"2024-09-09T11:27:52.923235Z","iopub.status.idle":"2024-09-09T11:28:07.337665Z","shell.execute_reply.started":"2024-09-09T11:27:52.923198Z","shell.execute_reply":"2024-09-09T11:28:07.336669Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T11:28:10.747300Z","iopub.execute_input":"2024-09-09T11:28:10.748146Z","iopub.status.idle":"2024-09-09T11:28:13.889764Z","shell.execute_reply.started":"2024-09-09T11:28:10.748101Z","shell.execute_reply":"2024-09-09T11:28:13.888895Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c98c05689d4b65af8ab39bee63cd18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824d8d03728c45a4ae115d5b28630d16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50be4dc8bc89483b877d811f5c0fb5d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230718271b774017b014c9ba35ddf567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3eaa8177d1b4c3bb810ae7c988d57ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09970e81e06545e2b3b7b2127fe76598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2833f32c0f4d47b5a94d85cb7fca6bdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d02221d93d044128ee26e7a290c8bec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64b04a387594c458c94b343d907f6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ebb36e772d41169a4efc40bd468748"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e554b492c9748299175a3cc1f18dd4b"}},"metadata":{}}]},{"cell_type":"code","source":"# sentences to encode\nsentences = [\n    \"I am happy.\",\n    \"I am sad.\",\n    \"I am content.\",\n    \"I am not happy.\",\n    \"I am not sad.\"\n]\n\n# embeddings\nembeddings = model.encode(sentences)\n\n# embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T11:28:18.107330Z","iopub.execute_input":"2024-09-09T11:28:18.108371Z","iopub.status.idle":"2024-09-09T11:28:18.228661Z","shell.execute_reply.started":"2024-09-09T11:28:18.108308Z","shell.execute_reply":"2024-09-09T11:28:18.227595Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ceaecd50cf45ab979e74339d4dd29f"}},"metadata":{}},{"name":"stdout","text":"tensor([[1.0000, 0.4128, 0.5024, 0.7230, 0.4712],\n        [0.4128, 1.0000, 0.3096, 0.4911, 0.7424],\n        [0.5024, 0.3096, 1.0000, 0.4433, 0.3470],\n        [0.7230, 0.4911, 0.4433, 1.0000, 0.6147],\n        [0.4712, 0.7424, 0.3470, 0.6147, 1.0000]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Part 3. Examining Llama2 predictions","metadata":{}},{"cell_type":"code","source":"# !pip install -U bitsandbytes\n\n## after installing it, you may need to 'stop session' and run it again from here","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:04:54.338067Z","iopub.execute_input":"2024-09-09T16:04:54.338957Z","iopub.status.idle":"2024-09-09T16:04:54.343399Z","shell.execute_reply.started":"2024-09-09T16:04:54.338910Z","shell.execute_reply":"2024-09-09T16:04:54.342264Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:04:45.683965Z","iopub.execute_input":"2024-09-09T16:04:45.684479Z","iopub.status.idle":"2024-09-09T16:04:51.020949Z","shell.execute_reply.started":"2024-09-09T16:04:45.684414Z","shell.execute_reply":"2024-09-09T16:04:51.019842Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nquant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False)\n\n# Pre-trained LLAMA2 from Hugging Face hub\nbase_model = \"NousResearch/Llama-2-7b-chat-hf\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=quant_config, device_map={\"\": 0})\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:04:59.485792Z","iopub.execute_input":"2024-09-09T16:04:59.486229Z","iopub.status.idle":"2024-09-09T16:06:28.980805Z","shell.execute_reply.started":"2024-09-09T16:04:59.486183Z","shell.execute_reply":"2024-09-09T16:06:28.979700Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c181760ffa504a208b6a4800f9329915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde02612fc66454a9b66ce37b43b7e24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d095f1dfb3482fb1fec3523d63d67b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c4563846444100bed0352773d4f7f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff6f83fd129d47c58b86158c51778aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93f77a3ee024bd298ee9c88e215d9d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc94b64d0d74c92b49f2b40a1db8c02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f1bfa235bb442ba192aae629e4fc28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759580ebeead473aacf268ec48dc25e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6997ca9d5c334fd99e451c10ddf9b084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb99ac1a953240f1bc4bc12e0e15aedf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d2d19a9a9994a94b5a3fe52a7aa8b4a"}},"metadata":{}}]},{"cell_type":"code","source":"# examining model structure\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:09:55.082795Z","iopub.execute_input":"2024-09-09T16:09:55.083455Z","iopub.status.idle":"2024-09-09T16:09:55.096547Z","shell.execute_reply.started":"2024-09-09T16:09:55.083403Z","shell.execute_reply":"2024-09-09T16:09:55.095211Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# generating text with a prompt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprompt = \"Who is Ferdinand de Saussure?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\ngenerate_ids = model.generate(inputs.input_ids.to(device), max_length=100)\noutput = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:09:59.584200Z","iopub.execute_input":"2024-09-09T16:09:59.584925Z","iopub.status.idle":"2024-09-09T16:10:21.947525Z","shell.execute_reply.started":"2024-09-09T16:09:59.584887Z","shell.execute_reply":"2024-09-09T16:10:21.946417Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Who is Ferdinand de Saussure?\n everybody who has studied linguistics knows that the father of modern linguistics is Ferdinand de Saussure. He was a Swiss linguist who lived from 1857 to 1913. Saussure is considered the founder of modern linguistics because he laid the groundwork for the field as we know it today.\n\nSaussure's most important work is his book \"Course in General Linguistics,\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Getting probabilities for the next word\n\nBased on [this Reddit thread](https://www.reddit.com/r/LocalLLaMA/comments/1b6xbg9/displayingreturning_probabilitieslogprobs_of_next/?rdt=37394)","metadata":{}},{"cell_type":"code","source":"input_string = \"I am\"\n\n# tokenize input\ninput_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n\n# get model predictions (logits: values before they are turned into probabilities via softmax)\nlogits = model(input_ids).logits\n\n# last projection: predicted next word after input\nlogits = logits[-1, -1]\n\n# change logits to probabilities via softmax\nprobs = torch.nn.functional.softmax(logits, dim=-1)\n\n# top 10 most probable predicted words\nprobs, ids = torch.topk(probs, 10)\n\n# convert token ids to text tokens\ntexts = tokenizer.convert_ids_to_tokens(ids)\n\n# print probabilities + tokens\nfor prob, text in zip(probs, texts):\n    print(f\"{prob:.4f}: \\\"{text}\\\"\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T15:32:47.033012Z","iopub.execute_input":"2024-09-09T15:32:47.033882Z","iopub.status.idle":"2024-09-09T15:32:47.319598Z","shell.execute_reply.started":"2024-09-09T15:32:47.033828Z","shell.execute_reply":"2024-09-09T15:32:47.318551Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0.2253: \"‚ñÅa\"\n0.0631: \"‚ñÅpleased\"\n0.0531: \"‚ñÅnot\"\n0.0503: \"‚ñÅso\"\n0.0454: \"‚ñÅthr\"\n0.0420: \"‚ñÅan\"\n0.0315: \"‚ñÅwriting\"\n0.0255: \"‚ñÅexcited\"\n0.0203: \"‚ñÅgrateful\"\n0.0183: \"‚ñÅhappy\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Getting predictions from different Llama2 layers\n\nBased on this [code](https://github.com/nrimsky/LM-exp/blob/main/intermediate_decoding/intermediate_decoding.ipynb) with an associated [post](https://www.lesswrong.com/posts/fJE6tscjGRPnK8C2C/decoding-intermediate-activations-in-llama-2-7b)","metadata":{}},{"cell_type":"code","source":"class AttnWrapper(torch.nn.Module):\n    def __init__(self, attn):\n        super().__init__()\n        self.attn = attn\n        self.activations = None\n        self.add_tensor = None\n\n    def forward(self, *args, **kwargs):\n        output = self.attn(*args, **kwargs)\n        if self.add_tensor is not None:\n            output = (output[0] + self.add_tensor,)+output[1:]\n        self.activations = output[0]\n        return output\n\n    def reset(self):\n        self.activations = None\n        self.add_tensor = None\n\n\nclass BlockOutputWrapper(torch.nn.Module):\n    def __init__(self, block, unembed_matrix, norm):\n        super().__init__()\n        self.block = block\n        self.unembed_matrix = unembed_matrix\n        self.norm = norm\n\n        self.block.self_attn = AttnWrapper(self.block.self_attn)\n        self.post_attention_layernorm = self.block.post_attention_layernorm\n\n        self.attn_mech_output_unembedded = None\n        self.intermediate_res_unembedded = None\n        self.mlp_output_unembedded = None\n        self.block_output_unembedded = None\n\n    def forward(self, *args, **kwargs):\n        output = self.block(*args, **kwargs)\n        self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n        attn_output = self.block.self_attn.activations\n        self.attn_mech_output_unembedded = self.unembed_matrix(self.norm(attn_output))\n        attn_output += args[0]\n        self.intermediate_res_unembedded = self.unembed_matrix(self.norm(attn_output))\n        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n        self.mlp_output_unembedded = self.unembed_matrix(self.norm(mlp_output))\n        return output\n\n    def attn_add_tensor(self, tensor):\n        self.block.self_attn.add_tensor = tensor\n\n    def reset(self):\n        self.block.self_attn.reset()\n\n    def get_attn_activations(self):\n        return self.block.self_attn.activations\n\n\nclass Llama7BHelper:\n    def __init__(self):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.tokenizer = tokenizer\n        self.model = model\n        for i, layer in enumerate(self.model.model.layers):\n            self.model.model.layers[i] = BlockOutputWrapper(layer, self.model.lm_head, self.model.model.norm)\n\n    def generate_text(self, prompt, max_length=100):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_length=max_length)\n        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\n    def get_logits(self, prompt):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        with torch.no_grad():\n          logits = self.model(inputs.input_ids.to(self.device)).logits\n          return logits\n\n    def set_add_attn_output(self, layer, add_output):\n        self.model.model.layers[layer].attn_add_tensor(add_output)\n\n    def get_attn_activations(self, layer):\n        return self.model.model.layers[layer].get_attn_activations()\n\n    def reset_all(self):\n        for layer in self.model.model.layers:\n            layer.reset()\n            \n    def print_decoded_activations(self, decoded_activations, label, topk=10):\n        softmaxed = torch.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n        values, indices = torch.topk(softmaxed, topk)\n        probs_percent = [int(v * 100) for v in values.tolist()]\n        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n        print(label, list(zip(tokens, probs_percent)))\n\n    def decode_all_layers(self, text, topk=10, print_attn_mech=True, print_intermediate_res=True, print_mlp=True, print_block=True):\n        self.get_logits(text)\n        for i, layer in enumerate(self.model.model.layers):\n            print(f'Layer {i}: Decoded intermediate outputs')\n            if print_attn_mech:\n                self.print_decoded_activations(layer.attn_mech_output_unembedded, 'Attention mechanism', topk=topk)\n            if print_intermediate_res:\n                self.print_decoded_activations(layer.intermediate_res_unembedded, 'Intermediate residual stream', topk=topk)\n            if print_mlp:\n                self.print_decoded_activations(layer.mlp_output_unembedded, 'MLP output', topk=topk)\n            if print_block:\n                self.print_decoded_activations(layer.block_output_unembedded, 'Block output', topk=topk)\n            print()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:11:50.697423Z","iopub.execute_input":"2024-09-09T16:11:50.697864Z","iopub.status.idle":"2024-09-09T16:11:50.723800Z","shell.execute_reply.started":"2024-09-09T16:11:50.697827Z","shell.execute_reply":"2024-09-09T16:11:50.722611Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"helper = Llama7BHelper()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:11:55.742879Z","iopub.execute_input":"2024-09-09T16:11:55.743664Z","iopub.status.idle":"2024-09-09T16:11:55.752135Z","shell.execute_reply.started":"2024-09-09T16:11:55.743605Z","shell.execute_reply":"2024-09-09T16:11:55.750921Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"helper.decode_all_layers('Colorless green')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:11:59.824322Z","iopub.execute_input":"2024-09-09T16:11:59.825232Z","iopub.status.idle":"2024-09-09T16:12:00.430689Z","shell.execute_reply.started":"2024-09-09T16:11:59.825189Z","shell.execute_reply":"2024-09-09T16:12:00.429563Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Layer 0: Decoded intermediate outputs\nAttention mechanism [('solution', 0), ('loose', 0), ('berga', 0), ('', 0), ('first', 0), ('Private', 0), ('itself', 0), ('applied', 0), ('h', 0), ('–ø—É—Ç–∞', 0)]\nIntermediate residual stream [('ery', 8), ('berga', 2), ('‡ßã', 0), ('Fritz', 0), ('SM', 0), ('SM', 0), ('solution', 0), ('aram', 0), ('hill', 0), ('ƒái', 0)]\nMLP output [('Kontrola', 2), ('Êòå', 0), ('Sito', 0), ('Â¥é', 0), ('–ø—É—Ç–∞', 0), ('pr√ºfe', 0), ('Èò≥', 0), ('‰π°', 0), ('mez', 0), ('bezeichneter', 0)]\nBlock output [('ery', 6), ('berga', 0), ('–ø—É—Ç–∞', 0), ('Êòå', 0), ('‡ßã', 0), ('mez', 0), ('Apple', 0), ('Èò≥', 0), ('Kontrola', 0), ('ƒö', 0)]\n\nLayer 1: Decoded intermediate outputs\nAttention mechanism [('r√©f√©rences', 1), ('chev', 0), ('isz', 0), ('rot', 0), ('embar', 0), ('–ù–∞—Å–µ', 0), ('imedia', 0), ('Àá', 0), ('older', 0), ('r√©seau', 0)]\nIntermediate residual stream [('ery', 20), ('print', 1), ('Au√üer', 1), ('wich', 0), ('mez', 0), ('mark', 0), ('ija', 0), ('dear', 0), ('impression', 0), ('l√°', 0)]\nMLP output [('apers', 2), ('mez', 1), ('screen', 0), ('Screen', 0), ('tml', 0), ('ock', 0), ('wich', 0), ('len', 0), ('house', 0), ('ville', 0)]\nBlock output [('ery', 13), ('wich', 4), ('mark', 1), ('dear', 1), ('print', 0), ('mez', 0), ('len', 0), ('house', 0), ('multicol', 0), ('mez', 0)]\n\nLayer 2: Decoded intermediate outputs\nAttention mechanism [('aterra', 1), ('icture', 1), ('√°j', 0), ('LCCN', 0), ('crew', 0), ('≈Çow', 0), ('redirects', 0), ('zia≈Ç', 0), ('hai', 0), ('aster', 0)]\nIntermediate residual stream [('wich', 8), ('ery', 7), ('stone', 1), ('mark', 1), ('mez', 1), ('dear', 0), ('Anto', 0), ('house', 0), ('print', 0), ('flex', 0)]\nMLP output [('ilde', 1), ('noise', 0), ('aris', 0), ('agens', 0), ('imit', 0), ('bil', 0), ('hus', 0), ('–≤—ñ—Ç', 0), ('hab', 0), ('Ru', 0)]\nBlock output [('ery', 12), ('wich', 4), ('mez', 1), ('stone', 0), ('dear', 0), ('house', 0), ('apers', 0), ('eye', 0), ('jack', 0), ('unya', 0)]\n\nLayer 3: Decoded intermediate outputs\nAttention mechanism [('G√©', 1), ('Summary', 1), ('–•—Ä–æ–Ω–æ–ª–æ–≥–∏—ò–∞', 0), ('nitt', 0), ('ikz', 0), ('atra', 0), ('Hou', 0), ('terior', 0), ('lacht', 0), ('WT', 0)]\nIntermediate residual stream [('ery', 3), ('house', 0), ('wich', 0), ('mez', 0), ('back', 0), ('arrow', 0), ('stone', 0), ('√æ', 0), ('lass', 0), ('painted', 0)]\nMLP output [('iser', 2), ('itel', 1), ('uns', 1), ('Portug', 0), ('erson', 0), ('Gem', 0), ('iso', 0), ('own', 0), ('Sund', 0), ('jon', 0)]\nBlock output [('ery', 3), ('wich', 2), ('lass', 1), ('stone', 0), ('back', 0), ('house', 0), ('print', 0), ('ock', 0), ('lee', 0), ('itel', 0)]\n\nLayer 4: Decoded intermediate outputs\nAttention mechanism [('mi', 1), ('amba', 0), ('aterra', 0), ('oka', 0), ('Bay', 0), ('plug', 0), ('cible', 0), ('Sank', 0), ('Mi', 0), ('Ghost', 0)]\nIntermediate residual stream [('lass', 3), ('wich', 1), ('Bay', 0), ('Sar', 0), ('√¢tre', 0), ('gem', 0), ('istes', 0), ('narrow', 0), ('Sob', 0), ('shame', 0)]\nMLP output [('Î≥¥', 5), ('vex', 2), ('arc', 2), ('sted', 1), ('alu', 1), ('Àá', 1), ('ption', 0), ('--+', 0), ('site', 0), ('ÁÇπ', 0)]\nBlock output [('wich', 3), ('ËØù', 2), ('bru', 0), ('—Ü–µ–Ω', 0), ('lee', 0), ('ish', 0), ('iono', 0), ('ClassLoader', 0), ('Ë©±', 0), ('—á–µ–Ω–∫–æ', 0)]\n\nLayer 5: Decoded intermediate outputs\nAttention mechanism [('erei', 4), ('esterni', 4), ('ÔøΩ', 2), ('ethe', 1), ('Ë¥•', 0), ('rok', 0), ('ipage', 0), ('«ê', 0), ('itul', 0), ('ilon', 0)]\nIntermediate residual stream [('ËØù', 3), ('wich', 1), ('bru', 1), ('Ë©±', 0), ('—á–µ–Ω–∫–æ', 0), ('lee', 0), ('ClassLoader', 0), ('ish', 0), ('—Ü–µ–Ω', 0), ('erei', 0)]\nMLP output [('vel', 2), ('Pen', 0), ('tera', 0), ('–º', 0), ('Publish', 0), ('pen', 0), ('–≥–µ', 0), ('√§ck', 0), ('ERR', 0), ('eign', 0)]\nBlock output [('wich', 2), ('ufen', 1), ('ifen', 1), ('erei', 1), ('thumb', 0), ('bru', 0), ('uso', 0), ('ËØù', 0), ('eni', 0), ('hou', 0)]\n\nLayer 6: Decoded intermediate outputs\nAttention mechanism [('perty', 3), ('phon', 3), ('icode', 1), ('sigu', 0), ('√∂l', 0), ('—Ö—É', 0), ('raph', 0), ('ienne', 0), ('orum', 0), ('paste', 0)]\nIntermediate residual stream [('bru', 1), ('—Ü–µ–Ω', 0), ('ioni', 0), ('ifen', 0), ('wich', 0), ('imation', 0), ('ufen', 0), ('uso', 0), ('erei', 0), ('laps', 0)]\nMLP output [('idel', 1), ('NR', 0), ('Mitchell', 0), ('rez', 0), ('cen', 0), ('asi', 0), ('ierra', 0), ('outer', 0), ('walls', 0), ('unix', 0)]\nBlock output [('wich', 2), ('—Ü–µ–Ω', 0), ('erei', 0), ('Bent', 0), ('Apple', 0), ('iev', 0), ('hou', 0), ('ouvel', 0), ('imation', 0), ('pent', 0)]\n\nLayer 7: Decoded intermediate outputs\nAttention mechanism [('vba', 0), ('zar', 0), ('–∑–≤–æ', 0), ('rijk', 0), ('visual', 0), ('Bit', 0), ('—É–¥–æ', 0), ('arina', 0), ('vern', 0), ('ponse', 0)]\nIntermediate residual stream [('wich', 1), ('pent', 0), ('Bent', 0), ('—Ü–µ–Ω', 0), ('ouvel', 0), ('mail', 0), ('hou', 0), ('bru', 0), ('laps', 0), ('erei', 0)]\nMLP output [('aucoup', 2), ('Cong', 1), ('a√±a', 0), ('thick', 0), ('c√≠', 0), ('Academy', 0), ('eles', 0), ('cock', 0), ('defin', 0), ('–≥–∏', 0)]\nBlock output [('wich', 6), ('uso', 0), ('–≥–æ', 0), ('ufen', 0), ('jpeg', 0), ('Apple', 0), ('—Ü–µ–Ω', 0), ('apple', 0), ('hook', 0), ('Fu√ü', 0)]\n\nLayer 8: Decoded intermediate outputs\nAttention mechanism [('mor', 3), ('mor', 1), ('Cult', 1), ('wan', 1), ('i√£o', 0), ('scriptstyle', 0), ('√§hr', 0), ('uta', 0), ('nou', 0), ('ymnas', 0)]\nIntermediate residual stream [('wich', 8), ('uso', 1), ('–≥–æ', 0), ('clos', 0), ('pent', 0), ('imation', 0), ('ifen', 0), ('jpeg', 0), ('apple', 0), ('ufen', 0)]\nMLP output [('atura', 1), ('emet', 1), ('Sar', 0), ('Porto', 0), ('cho', 0), ('gov', 0), ('√ª', 0), ('‰ø°', 0), ('picture', 0), ('uni', 0)]\nBlock output [('jpeg', 1), ('wich', 0), ('emet', 0), ('ef', 0), ('onk', 0), ('gi', 0), ('Sar', 0), ('mail', 0), ('uso', 0), ('chor', 0)]\n\nLayer 9: Decoded intermediate outputs\nAttention mechanism [('ECK', 3), ('Jenkins', 1), ('andas', 1), ('Days', 1), ('ƒÖ≈º', 1), ('aden', 0), ('LIM', 0), ('√©c', 0), ('black', 0), ('tec', 0)]\nIntermediate residual stream [('gi', 1), ('jpeg', 0), ('uso', 0), ('wich', 0), ('ocal', 0), ('onk', 0), ('bl', 0), ('emit', 0), ('Sar', 0), ('istre', 0)]\nMLP output [('attan', 1), ('Zar', 1), ('kl', 1), ('fatal', 1), ('full', 0), ('aden', 0), ('anta', 0), ('—Å—Å–∏', 0), ('map', 0), ('optional', 0)]\nBlock output [('Zar', 1), ('flex', 1), ('anta', 0), ('ku', 0), ('Sar', 0), ('jpeg', 0), ('kre', 0), ('wich', 0), ('onk', 0), ('uso', 0)]\n\nLayer 10: Decoded intermediate outputs\nAttention mechanism [('√à', 3), ('orum', 2), ('bay', 1), ('aget', 0), ('velocity', 0), ('Dean', 0), ('zew', 0), ('azar', 0), ('ALSE', 0), ('ername', 0)]\nIntermediate residual stream [('flex', 1), ('kre', 1), ('anta', 0), ('ku', 0), ('onk', 0), ('jpeg', 0), ('rare', 0), ('Sar', 0), ('uso', 0), ('Zar', 0)]\nMLP output [('favor', 2), ('mine', 1), ('White', 1), ('Ï†ú', 1), ('Mine', 0), ('reb', 0), ('MY', 0), ('inkel', 0), ('ahr', 0), ('ccc', 0)]\nBlock output [('Sar', 3), ('flex', 0), ('ku', 0), ('uso', 0), ('anta', 0), ('onk', 0), ('endo', 0), ('wich', 0), ('Mill', 0), ('bl', 0)]\n\nLayer 11: Decoded intermediate outputs\nAttention mechanism [('merk', 1), ('tor', 0), ('revol', 0), ('Gh', 0), ('rak', 0), ('–æ—Ç–µ', 0), ('tact', 0), ('reu', 0), ('Kin', 0), ('etzt', 0)]\nIntermediate residual stream [('Sar', 2), ('ku', 1), ('anta', 0), ('ifen', 0), ('wich', 0), ('Mill', 0), ('onk', 0), ('uso', 0), ('flex', 0), ('kre', 0)]\nMLP output [('wor', 3), ('ster', 2), ('hmen', 0), ('oure', 0), ('bank', 0), ('interests', 0), ('ument', 0), ('imore', 0), ('—Å–ª–∏', 0), ('neh', 0)]\nBlock output [('Sar', 3), ('wich', 1), ('cla', 0), ('Mill', 0), ('ku', 0), ('bl', 0), ('ifen', 0), ('patch', 0), ('Fun', 0), ('onk', 0)]\n\nLayer 12: Decoded intermediate outputs\nAttention mechanism [('avia', 1), ('ËÇ≤', 1), ('ggi', 1), ('Mail', 1), ('utto', 0), ('cust', 0), ('Bis', 0), ('appa', 0), ('ori', 0), ('FIX', 0)]\nIntermediate residual stream [('wich', 1), ('ku', 1), ('Sar', 1), ('cla', 1), ('bl', 0), ('ifen', 0), ('win', 0), ('Mac', 0), ('Mill', 0), ('meant', 0)]\nMLP output [('–≤–∏–¥', 5), ('UMN', 0), ('ello', 0), ('born', 0), ('ennis', 0), ('bay', 0), ('G√ºn', 0), ('eed', 0), ('UTE', 0), ('–≥–æ–≤', 0)]\nBlock output [('cla', 2), ('wich', 1), ('Sar', 1), ('ug', 0), ('ifen', 0), ('clause', 0), ('meant', 0), ('ku', 0), ('kre', 0), ('bl', 0)]\n\nLayer 13: Decoded intermediate outputs\nAttention mechanism [('oltre', 2), ('Autres', 0), ('G√©', 0), ('lands', 0), ('enberg', 0), ('mul', 0), ('repeating', 0), ('esp', 0), ('femin', 0), ('enburg', 0)]\nIntermediate residual stream [('wich', 2), ('ug', 1), ('cla', 1), ('Sar', 0), ('ifen', 0), ('meant', 0), ('ku', 0), ('clause', 0), ('lands', 0), ('endo', 0)]\nMLP output [('iech', 2), ('edia', 1), ('Mittel', 0), ('–æ—Ä—É', 0), ('nick', 0), ('Herzog', 0), ('rikt', 0), ('TY', 0), ('esses', 0), ('Medi', 0)]\nBlock output [('wich', 4), ('Sar', 1), ('Kra', 1), ('kre', 0), ('wood', 0), ('Zar', 0), ('cla', 0), ('lands', 0), ('ug', 0), ('endo', 0)]\n\nLayer 14: Decoded intermediate outputs\nAttention mechanism [('Ej', 3), ('ct', 0), ('olf', 0), ('form', 0), ('oru', 0), ('libre', 0), ('pool', 0), ('Tem', 0), ('ctu', 0), ('No', 0)]\nIntermediate residual stream [('wich', 3), ('Sar', 1), ('kre', 1), ('endo', 0), ('Kra', 0), ('lands', 0), ('drum', 0), ('wood', 0), ('ele', 0), ('ug', 0)]\nMLP output [('col', 5), ('ery', 3), ('antry', 2), ('ish', 1), ('fach', 1), ('estr', 0), ('edy', 0), ('xsd', 0), ('esi', 0), ('Sky', 0)]\nBlock output [('wich', 12), ('ish', 1), ('UC', 0), ('per', 0), ('ku', 0), ('bid', 0), ('wid', 0), ('iful', 0), ('fin', 0), ('Sar', 0)]\n\nLayer 15: Decoded intermediate outputs\nAttention mechanism [('UC', 1), ('kre', 1), ('rh', 1), ('gla', 0), ('pod', 0), ('vote', 0), ('Proposition', 0), ('√®ncies', 0), ('agua', 0), ('ymen', 0)]\nIntermediate residual stream [('wich', 10), ('UC', 3), ('ish', 1), ('kre', 0), ('fin', 0), ('ku', 0), ('wid', 0), ('lbl', 0), ('Haz', 0), ('ifen', 0)]\nMLP output [('Footnote', 1), ('—Ç–µ—Ç', 1), ('Bedeut', 0), ('l√†', 0), ('√•rs', 0), ('inks', 0), ('merk', 0), ('StackTrace', 0), ('imoine', 0), ('Einzel', 0)]\nBlock output [('wich', 19), ('UC', 2), ('ish', 1), ('hos', 0), ('spaces', 0), ('ery', 0), ('mail', 0), ('lbl', 0), ('Heinrich', 0), ('wid', 0)]\n\nLayer 16: Decoded intermediate outputs\nAttention mechanism [('eme', 1), ('Blues', 1), ('ols', 1), ('sem', 0), ('Bass', 0), ('sem', 0), ('Sem', 0), ('icol', 0), ('Chem', 0), ('blue', 0)]\nIntermediate residual stream [('wich', 24), ('UC', 3), ('hos', 0), ('ish', 0), ('spaces', 0), ('mail', 0), ('lbl', 0), ('eme', 0), ('lands', 0), ('fin', 0)]\nMLP output [('rug', 2), ('atus', 0), ('afka', 0), ('DA', 0), ('zes', 0), ('Ban', 0), ('vention', 0), ('·ªù', 0), ('developer', 0), ('actor', 0)]\nBlock output [('wich', 10), ('UC', 3), ('ish', 1), ('hos', 1), ('ery', 0), ('spaces', 0), ('fin', 0), ('apple', 0), ('wood', 0), ('eme', 0)]\n\nLayer 17: Decoded intermediate outputs\nAttention mechanism [('‚ô¶', 2), ('Pho', 1), (':@', 1), ('–ø–æ—Ä', 0), ('„Çº', 0), ('–∫–∞—è', 0), ('r√©f√©rences', 0), ('punkt', 0), ('testing', 0), ('Ses', 0)]\nIntermediate residual stream [('wich', 9), ('UC', 4), ('hos', 1), ('ish', 0), ('fin', 0), ('traffic', 0), ('wood', 0), ('col', 0), ('ery', 0), ('HL', 0)]\nMLP output [('estra', 1), ('meno', 0), ('emas', 0), ('back', 0), ('emb', 0), ('olean', 0), ('ainer', 0), ('UNION', 0), ('dot', 0), ('Ãç', 0)]\nBlock output [('wich', 14), ('UC', 6), ('ish', 5), ('ery', 2), ('fin', 0), ('hos', 0), ('azionale', 0), ('Ce', 0), ('dot', 0), ('HL', 0)]\n\nLayer 18: Decoded intermediate outputs\nAttention mechanism [('outer', 1), ('bres', 0), ('ombres', 0), ('trans', 0), ('res', 0), ('mez', 0), ('obi', 0), ('aca', 0), ('wand', 0), ('fred', 0)]\nIntermediate residual stream [('wich', 10), ('UC', 6), ('ish', 4), ('ery', 1), ('dot', 0), ('hos', 0), ('HL', 0), ('azionale', 0), ('kre', 0), ('fin', 0)]\nMLP output [('awa', 4), ('√§s', 2), ('ery', 1), ('Bes', 1), ('Hell', 1), ('—Å–∫—É', 1), ('vice', 1), ('–µ–¥–µ', 0), ('estaven', 0), ('ƒõn√≠', 0)]\nBlock output [('wich', 19), ('ery', 10), ('ish', 7), ('UC', 6), ('dot', 0), ('eme', 0), ('eyes', 0), ('Bean', 0), ('ovis', 0), ('beans', 0)]\n\nLayer 19: Decoded intermediate outputs\nAttention mechanism [('od', 1), ('Person', 0), ('plex', 0), ('otrop', 0), ('Ress', 0), ('fg', 0), ('ËâØ', 0), ('dom', 0), ('oracle', 0), ('quer', 0)]\nIntermediate residual stream [('wich', 16), ('ery', 7), ('UC', 6), ('ish', 6), ('dot', 1), ('eme', 0), ('beans', 0), ('Ce', 0), ('eyes', 0), ('Bean', 0)]\nMLP output [('thumb', 18), ('buch', 2), ('erm', 0), ('lon', 0), ('Tamb', 0), ('zie', 0), ('–æ–ª–æ', 0), ('est', 0), ('alet', 0), ('lik', 0)]\nBlock output [('ish', 29), ('wich', 15), ('ery', 5), ('UC', 3), ('Ce', 0), ('beans', 0), ('house', 0), ('isen', 0), ('ce', 0), ('Bean', 0)]\n\nLayer 20: Decoded intermediate outputs\nAttention mechanism [('color', 75), ('color', 11), ('Color', 11), ('Color', 0), ('colors', 0), ('colored', 0), ('colors', 0), ('Colors', 0), ('colour', 0), ('Ëâ≤', 0)]\nIntermediate residual stream [('ish', 28), ('wich', 16), ('ery', 4), ('UC', 3), ('Ce', 0), ('light', 0), ('beans', 0), ('house', 0), ('isen', 0), ('transparent', 0)]\nMLP output [('gre', 2), ('jul', 0), ('v√°', 0), ('green', 0), ('Green', 0), ('town', 0), ('–ü—É', 0), ('proper', 0), ('bel', 0), ('Ready', 0)]\nBlock output [('ish', 39), ('wich', 7), ('UC', 2), ('ery', 2), ('spaces', 0), ('house', 0), ('Ce', 0), ('isen', 0), ('transparent', 0), ('ishes', 0)]\n\nLayer 21: Decoded intermediate outputs\nAttention mechanism [('color', 2), ('odon', 1), ('uba', 0), ('carbon', 0), ('Color', 0), ('arda', 0), ('forces', 0), ('obox', 0), ('aft', 0), ('wagen', 0)]\nIntermediate residual stream [('ish', 37), ('wich', 7), ('UC', 2), ('ery', 1), ('spaces', 0), ('house', 0), ('Ce', 0), ('light', 0), ('isen', 0), ('alg', 0)]\nMLP output [('len', 1), ('method', 1), ('Reset', 0), ('typeof', 0), ('Graph', 0), ('Graph', 0), ('√ºs', 0), ('ÔøΩ', 0), ('team', 0), ('law', 0)]\nBlock output [('ish', 30), ('wich', 4), ('ery', 2), ('UC', 1), ('spaces', 0), ('jub', 0), ('field', 0), ('Bean', 0), ('light', 0), ('ough', 0)]\n\nLayer 22: Decoded intermediate outputs\nAttention mechanism [('√ºcke', 0), ('ÔøΩ', 0), ('enes', 0), ('pol', 0), ('red', 0), ('Í∏∞', 0), ('orf', 0), ('dorf', 0), ('Route', 0), ('√ü', 0)]\nIntermediate residual stream [('ish', 25), ('wich', 3), ('ery', 2), ('UC', 1), ('esi', 0), ('field', 0), ('spaces', 0), ('gr', 0), ('ces', 0), ('fin', 0)]\nMLP output [('–∫–ª–∞–¥', 1), ('atif', 0), ('house', 0), ('gets', 0), ('dur', 0), ('efe', 0), ('dom', 0), ('hash', 0), ('–ø–æ–ª—å', 0), ('ƒ≠', 0)]\nBlock output [('ish', 16), ('wich', 2), ('house', 2), ('gr', 1), ('esi', 1), ('ery', 0), ('spaces', 0), ('alg', 0), ('UC', 0), ('ius', 0)]\n\nLayer 23: Decoded intermediate outputs\nAttention mechanism [('—Å–æ–≤', 0), ('—Ä–æ–≤', 0), ('–ü–æ–ø–∏—Å', 0), ('aria', 0), ('agen', 0), ('maz', 0), ('hina', 0), ('–•—Ä–æ–Ω–æ–ª–æ–≥–∏—ò–∞', 0), ('azar', 0), ('unda', 0)]\nIntermediate residual stream [('ish', 13), ('wich', 2), ('house', 1), ('gr', 1), ('ery', 1), ('spaces', 0), ('esi', 0), ('alg', 0), ('ough', 0), ('UC', 0)]\nMLP output [('kw', 8), ('ire', 1), ('sun', 1), ('Plant', 0), ('Otto', 0), ('–±—ñ—Ä', 0), ('lit', 0), ('ween', 0), ('iero', 0), ('aggio', 0)]\nBlock output [('ish', 14), ('house', 4), ('wich', 1), ('spaces', 1), ('lit', 1), ('light', 0), ('esi', 0), ('dawn', 0), ('alg', 0), ('gr', 0)]\n\nLayer 24: Decoded intermediate outputs\nAttention mechanism [('color', 57), ('color', 10), ('colors', 8), ('Color', 4), ('colour', 2), ('Color', 1), ('colours', 1), ('colored', 1), ('Colors', 1), ('colors', 0)]\nIntermediate residual stream [('ish', 16), ('house', 3), ('spaces', 1), ('lit', 1), ('wich', 1), ('light', 1), ('alg', 0), ('esi', 0), ('gr', 0), ('dawn', 0)]\nMLP output [('host', 1), ('esi', 1), ('ur', 1), ('l√°', 1), ('Gr', 0), ('host', 0), ('ilon', 0), ('herit', 0), ('idge', 0), ('perman', 0)]\nBlock output [('ish', 11), ('house', 8), ('esi', 4), ('lit', 2), ('ies', 1), ('gr', 0), ('spaces', 0), ('back', 0), ('wich', 0), ('ough', 0)]\n\nLayer 25: Decoded intermediate outputs\nAttention mechanism [('scal', 1), ('uen', 1), ('omp', 0), ('Sans', 0), ('–±–µ—Ä–≥', 0), ('curs', 0), ('mun', 0), ('hath', 0), ('oct', 0), ('dic', 0)]\nIntermediate residual stream [('ish', 10), ('house', 6), ('esi', 4), ('lit', 1), ('ies', 1), ('gr', 1), ('spaces', 0), ('wich', 0), ('ough', 0), ('ery', 0)]\nMLP output [('gerufen', 1), ('/~', 1), ('Dynamic', 0), ('iƒç', 0), ('eerd', 0), ('√∂s', 0), ('Convert', 0), ('schen', 0), ('rvm', 0), ('√°ny', 0)]\nBlock output [('ish', 5), ('house', 4), ('esi', 4), ('ery', 1), ('lit', 1), ('light', 1), ('wich', 1), ('gr', 0), ('oshi', 0), ('bean', 0)]\n\nLayer 26: Decoded intermediate outputs\nAttention mechanism [('only', 1), ('only', 1), ('Committee', 1), ('dt', 0), ('Ry', 0), ('stone', 0), ('Hou', 0), ('pc', 0), ('asa', 0), ('Bere', 0)]\nIntermediate residual stream [('ish', 3), ('esi', 3), ('house', 2), ('ery', 2), ('light', 1), ('lit', 0), ('bean', 0), ('las', 0), ('wich', 0), ('Dragon', 0)]\nMLP output [('Lib', 1), ('capital', 1), ('par', 0), ('em', 0), ('d', 0), ('Ax', 0), ('inha', 0), ('„Éë', 0), ('O', 0), ('–ø–∞—Ä', 0)]\nBlock output [('ish', 4), ('esi', 2), ('house', 2), ('light', 1), ('las', 1), ('leaf', 0), ('thumb', 0), ('hell', 0), ('alg', 0), ('gr', 0)]\n\nLayer 27: Decoded intermediate outputs\nAttention mechanism [('undle', 2), ('≈ºe', 1), ('acia', 1), ('j', 1), ('aben', 0), ('mod', 0), ('Variable', 0), ('Braun', 0), ('ugust', 0), ('Ram', 0)]\nIntermediate residual stream [('ish', 4), ('esi', 3), ('house', 2), ('light', 1), ('las', 0), ('ery', 0), ('thumb', 0), ('leaf', 0), ('alg', 0), ('flash', 0)]\nMLP output [('eken', 0), ('Jahrh', 0), ('cub', 0), ('–û—Ç–µ', 0), ('bbi', 0), ('ela', 0), ('Mys', 0), ('Rat', 0), ('√©l√©', 0), ('–ø—Ä–∞–≤–∏', 0)]\nBlock output [('ish', 5), ('esi', 2), ('las', 2), ('house', 0), ('ois', 0), ('flash', 0), ('light', 0), ('back', 0), ('gef', 0), ('bott', 0)]\n\nLayer 28: Decoded intermediate outputs\nAttention mechanism [('P', 22), ('S', 5), ('East', 2), ('West', 1), ('D√©', 0), ('Don', 0), ('DA', 0), ('Dic', 0), ('DA', 0), ('DM', 0)]\nIntermediate residual stream [('ish', 5), ('las', 2), ('esi', 2), ('flash', 0), ('house', 0), ('light', 0), ('ois', 0), ('back', 0), ('Pages', 0), ('Dragon', 0)]\nMLP output [('istrzost', 2), ('hill', 1), ('Hill', 1), ('ÔøΩ', 0), ('religion', 0), ('elle', 0), ('grammar', 0), ('umerate', 0), ('essa', 0), ('ince', 0)]\nBlock output [('ish', 4), ('esi', 1), ('gr', 1), ('las', 1), ('Dragon', 0), ('back', 0), ('house', 0), ('light', 0), ('fluid', 0), ('flash', 0)]\n\nLayer 29: Decoded intermediate outputs\nAttention mechanism [('dry', 1), ('lish', 0), ('8', 0), ('Press', 0), ('uen', 0), ('Las', 0), ('press', 0), ('gr', 0), ('meister', 0), ('ribu', 0)]\nIntermediate residual stream [('ish', 3), ('gr', 1), ('las', 1), ('esi', 1), ('back', 0), ('light', 0), ('fluid', 0), ('flash', 0), ('Dragon', 0), ('house', 0)]\nMLP output [('ide', 58), ('Ide', 29), ('IDE', 4), ('ide', 4), ('ideas', 1), ('IDE', 0), ('idea', 0), ('–∏–¥–µ', 0), ('ideal', 0), ('l', 0)]\nBlock output [('ideas', 6), ('ide', 6), ('ish', 3), ('las', 3), ('ies', 1), ('idea', 1), ('IDE', 1), ('Ide', 1), ('IDE', 0), ('gr', 0)]\n\nLayer 30: Decoded intermediate outputs\nAttention mechanism [('[', 0), ('...', 0), ('buy', 0), ('...', 0), ('`', 0), ('mens', 0), ('por', 0), ('–â', 0), ('Por', 0), ('aph', 0)]\nIntermediate residual stream [('ideas', 6), ('ide', 5), ('las', 2), ('ish', 2), ('ies', 1), ('Ide', 1), ('idea', 1), ('IDE', 1), ('IDE', 0), ('Las', 0)]\nMLP output [('T', 97), ('m', 0), ('V', 0), ('tr', 0), ('\\n', 0), ('(', 0), ('s', 0), ('H', 0), ('ch', 0), ('e', 0)]\nBlock output [('ideas', 8), ('las', 5), ('ish', 4), ('ide', 4), ('-', 3), ('d', 2), ('g', 2), ('\\n', 2), ('.', 1), ('ies', 1)]\n\nLayer 31: Decoded intermediate outputs\nAttention mechanism [('...', 1), ('...', 1), ('Sym', 1), ('geomet', 0), ('b', 0), ('ones', 0), ('seconds', 0), ('abeth', 0), ('uch', 0), ('lob', 0)]\nIntermediate residual stream [('las', 9), ('ideas', 6), ('-', 5), ('.', 3), ('d', 2), ('\\n', 2), ('g', 2), ('ish', 2), ('ide', 2), (',', 1)]\nMLP output [('ideas', 2), ('thoughts', 2), ('elli', 2), ('medal', 2), ('hill', 1), ('eggs', 1), ('curl', 1), ('hills', 0), ('clouds', 0), ('epo', 0)]\nBlock output [('ideas', 72), ('d', 1), ('is', 1), ('-', 0), ('.', 0), ('and', 0), ('g', 0), ('las', 0), ('eggs', 0), (',', 0)]\n\n","output_type":"stream"}]}]}